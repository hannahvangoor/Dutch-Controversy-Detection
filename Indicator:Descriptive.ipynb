{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import timeit\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.stats import entropy\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from sklearn.pipeline import Pipeline\n",
    "import os\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "import string\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import SnowballStemmer\n",
    "\n",
    "from nltk.stem import SnowballStemmer, WordNetLemmatizer\n",
    "\n",
    "# Clean and tokenize text\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import string\n",
    "\n",
    "nltk.download('stopwords') # download stopwords corpus\n",
    "nltk.download('punkt') # download punkt tokenizer\n",
    "\n",
    "# For linear regression\n",
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the reaction columns and total reactions column name\n",
    "reaction_columns = [\"like\", \"wow\", \"cares\", \"sad\", \"angry\", \"haha\"]\n",
    "total_reactions = \"reactions_count\"\n",
    "\n",
    "class EntropyCalculator(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, reaction_columns, total_reactions):\n",
    "        self.reaction_columns = reaction_columns\n",
    "        self.total_reactions = total_reactions\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        # Create a copy of the input DataFrame\n",
    "        X_transformed = X.copy()\n",
    "        \n",
    "        # Define a function to calculate the entropy\n",
    "        def calculate_entropy(row):\n",
    "            # Get the reaction counts for the row\n",
    "            counts = row[self.reaction_columns].values\n",
    "            # Normalize the counts to probabilities\n",
    "            eps = 1e-8  # a very small positive number\n",
    "            probabilities = (counts.astype(float) + eps) / (row[self.total_reactions] + len(self.reaction_columns) * eps)\n",
    "            \n",
    "            # Calculate the entropy\n",
    "            return entropy(probabilities)\n",
    "        \n",
    "        # Apply the entropy calculation function to each row\n",
    "        X_transformed[\"entropy\"] = X_transformed.apply(calculate_entropy, axis=1)\n",
    "        \n",
    "        return X_transformed\n",
    "\n",
    "\n",
    "# Custom transformer for descriptive statistics\n",
    "class DescriptiveStatistics(BaseEstimator, TransformerMixin):\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        print(X.describe())\n",
    "        return X\n",
    "\n",
    "# Custom transformer for plotting descriptive statistics\n",
    "class DescriptiveStatsPlotter(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, figsize=(10, 8)):\n",
    "        self.figsize = figsize\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        # Calculate descriptive statistics\n",
    "        desc_stats = X.describe()\n",
    "        \n",
    "        # Plot histogram for each numerical column\n",
    "        for col in desc_stats.columns:\n",
    "            if pd.api.types.is_numeric_dtype(X[col]):\n",
    "                fig, ax = plt.subplots(figsize=self.figsize)\n",
    "                sns.histplot(data=X, x=col, kde=True, color=\"skyblue\", alpha=0.8)\n",
    "                average = X[col].mean()\n",
    "        \n",
    "                # Add the average line\n",
    "                ax.axhline(average, color='red', linestyle='--', label='Average')\n",
    "                ax.set_xlabel(col.capitalize())\n",
    "                ax.set_ylabel(\"Frequency\")\n",
    "                ax.set_title(f\"Histogram of {col.capitalize()}\")\n",
    "                plt.show()\n",
    "        \n",
    "        # Plot boxplot for each numerical column\n",
    "        for col in desc_stats.columns:\n",
    "            if pd.api.types.is_numeric_dtype(X[col]):\n",
    "                fig, ax = plt.subplots(figsize=self.figsize)\n",
    "                sns.boxplot(data=X, x=col, color=\"skyblue\", width=0.5, fliersize=3)\n",
    "                ax.set_xlabel(col.capitalize())\n",
    "                ax.set_ylabel(\"Value\")\n",
    "                ax.set_title(f\"Boxplot of {col.capitalize()}\")\n",
    "                plt.show()\n",
    "        \n",
    "        # Plot countplot for each categorical column\n",
    "        for col in desc_stats.columns:\n",
    "            if pd.api.types.is_categorical_dtype(X[col]):\n",
    "                fig, ax = plt.subplots(figsize=self.figsize)\n",
    "                sns.countplot(data=X, x=col, color=\"skyblue\")\n",
    "                ax.set_xlabel(col.capitalize())\n",
    "                ax.set_ylabel(\"Count\")\n",
    "                ax.set_title(f\"Countplot of {col.capitalize()}\")\n",
    "                plt.show()\n",
    "        \n",
    "        return X\n",
    "    \n",
    "# Transformer for length analysis\n",
    "class LengthAnalysisTransformer(BaseEstimator, TransformerMixin):\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        X['word_count'] = X[\"content\"].apply(lambda x: len(str(x).split(\" \")))\n",
    "        X['char_count'] = X[\"content\"].apply(lambda x: sum(len(word) for word in str(x).split(\" \")))\n",
    "        X['sentence_count'] = X[\"content\"].apply(lambda x: len(str(x).split(\".\")))\n",
    "        X['avg_word_length'] = X['char_count'] / X['word_count']\n",
    "        X['avg_sentence_length'] = X['word_count'] / X['sentence_count']\n",
    "        return X\n",
    "\n",
    "class PreprocessTransformer(BaseEstimator, TransformerMixin):\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        # Drop rows with entropy score of 1.3 and higher\n",
    "        X = X[X['entropy'] < 1.3]\n",
    "        \n",
    "        # Delete rows with total reactions count of 30 and lower\n",
    "        X = X[X['reactions_count'] > 30]\n",
    "        \n",
    "        return X\n",
    "\n",
    "class ColumnAdder(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, col1, col2, new_col_name):\n",
    "        self.col1 = col1\n",
    "        self.col2 = col2\n",
    "        self.new_col_name = new_col_name\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        # Add column based on col1 and col2\n",
    "        X[self.new_col_name] = X[self.col1] + X[self.col2]\n",
    "        X = X.drop(columns=[self.col1, self.col2])\n",
    "\n",
    "        return X\n",
    "\n",
    "class SourceAppender(BaseEstimator, TransformerMixin):\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        X['content'] = X.apply(self.append_source_name, axis=1)\n",
    "        return X\n",
    "\n",
    "    def append_source_name(self, df):\n",
    "        if pd.notnull(df['content']):\n",
    "            return str(df['content']) + ' ' + df['name']\n",
    "        else:\n",
    "            return np.nan\n",
    "\n",
    "    \n",
    " # Download Dutch stopwords if not already downloaded\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Custom transformer for generating word clouds\n",
    "class WordCloudGenerator(BaseEstimator, TransformerMixin):\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        # Get Dutch stopwords\n",
    "        stopwords_nl = set(stopwords.words('dutch'))\n",
    "\n",
    "        # Filter the dataframe based on entropy scores\n",
    "        filtered_texts_low_entropy = X[X['entropy'].between(0.1, 0.7)]['content'].str.cat(sep=' ')\n",
    "        filtered_texts_high_entropy = X[X['entropy'].between(0.7, 1.2)]['content'].str.cat(sep=' ')\n",
    "\n",
    "        # Remove stopwords from the filtered texts\n",
    "        filtered_texts_low_entropy = ' '.join([word for word in filtered_texts_low_entropy.split() if word.lower() not in stopwords_nl])\n",
    "        filtered_texts_high_entropy = ' '.join([word for word in filtered_texts_high_entropy.split() if word.lower() not in stopwords_nl])\n",
    "\n",
    "        # Generate word clouds\n",
    "        wordcloud_low_entropy = WordCloud(background_color='white', colormap='Blues', width=800, height=400).generate(filtered_texts_low_entropy)\n",
    "        wordcloud_high_entropy = WordCloud(background_color='white', colormap='Reds', width=800, height=400).generate(filtered_texts_high_entropy)\n",
    "\n",
    "        # Plot the word clouds\n",
    "        plt.figure(figsize=(10, 5))\n",
    "        plt.subplot(1, 2, 1)\n",
    "        plt.imshow(wordcloud_low_entropy, interpolation='bilinear')\n",
    "        plt.title('Entropy score between 0.1 - 0.7')\n",
    "        plt.axis('off')\n",
    "\n",
    "        plt.subplot(1, 2, 2)\n",
    "        plt.imshow(wordcloud_high_entropy, interpolation='bilinear')\n",
    "        plt.title('Entropy score between 0.7 - 1.1')\n",
    "        plt.axis('off')\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "        # Return the input data unmodified\n",
    "        return X   \n",
    "\n",
    "# Custom transformer for analyzing entropy\n",
    "class EntropyAnalyzer(BaseEstimator, TransformerMixin):\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        # Sort the DataFrame by entropy in descending order to get the highest entropy posts\n",
    "        highest_entropy_posts = X.sort_values('entropy', ascending=False).head(5)\n",
    "\n",
    "        # Sort the DataFrame by entropy in ascending order to get the lowest entropy posts\n",
    "        lowest_entropy_posts = X.sort_values('entropy', ascending=True).head(5)\n",
    "\n",
    "        # Print the texts of the highest entropy posts\n",
    "        print(\"Texts of news posts with highest entropy:\")\n",
    "        for index, row in highest_entropy_posts.iterrows():\n",
    "            print(row['entropy'], row['content'])\n",
    "            print()  # Print an empty line between each post\n",
    "\n",
    "        # Print the texts of the lowest entropy posts\n",
    "        print(\"Texts of news posts with lowest entropy:\")\n",
    "        for index, row in lowest_entropy_posts.iterrows():\n",
    "            print(row['entropy'], row['content'])\n",
    "            print()  # Print an empty line between each post\n",
    "\n",
    "        # Return the input data unmodified\n",
    "        return X\n",
    "    \n",
    "def combine_csv_files(folder_path):\n",
    "    \"\"\"Combines CSV files in a folder with identical structures and creates a new ID column.\"\"\"\n",
    "    # Get all CSV files in the folder\n",
    "    csv_files = [f for f in os.listdir(folder_path) if f.endswith('.csv')]\n",
    "    # Read each CSV file and concatenate them\n",
    "    df_list = []\n",
    "    for file in csv_files:\n",
    "        filepath = os.path.join(folder_path, file)\n",
    "        df = pd.read_csv(filepath)\n",
    "        df_list.append(df)\n",
    "    combined_df = pd.concat(df_list, ignore_index=True)\n",
    "    # Create a new ID column\n",
    "    #combined_df['new_id'] = combined_df.index + 1\n",
    "    # Return the combined dataframe\n",
    "    return combined_df\n",
    "\n",
    "class CSVCombiner(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, folder_path):\n",
    "        self.folder_path = folder_path\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        combined_df = combine_csv_files(self.folder_path)\n",
    "        return combined_df\n",
    "    \n",
    "class CSVWriter(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, file_path):\n",
    "        self.file_path = file_path\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        X.to_csv(self.file_path, index=False)\n",
    "        return X\n",
    "    \n",
    "# Custom transformer for grouping and plotting entropy scores by news source\n",
    "class EntropyScorePlotter(BaseEstimator, TransformerMixin):\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        # Group the data by news source\n",
    "        grouped_df = X.groupby('name')\n",
    "\n",
    "        # Iterate over each news source\n",
    "        for source, group in grouped_df:\n",
    "            # Plot the histogram of entropy scores for the current news source\n",
    "            plt.figure(figsize=(8, 6))\n",
    "            plt.hist(group['entropy'], bins=10, edgecolor='black', alpha=0.75)\n",
    "            plt.xlabel('Entropy Score')\n",
    "            plt.ylabel('Frequency')\n",
    "            plt.title(f'{source}')\n",
    "            plt.xticks(rotation=45)\n",
    "            plt.show()\n",
    "\n",
    "        return X\n",
    "    \n",
    "# Transformer for reaction analysis    \n",
    "class ReactionAnalysisTransformer(BaseEstimator, TransformerMixin):\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        \n",
    "        # Group the DataFrame by 'name' column (news source) and calculate the sum of Facebook reactions\n",
    "        reaction_distribution = X.groupby('name')[['like', 'wow', 'cares', 'sad', 'angry', 'haha']].sum()\n",
    "        reaction_distribution['total'] = reaction_distribution.sum(axis=1)\n",
    "\n",
    "        # Calculate the total likes, total wows, etc.\n",
    "        total_likes = reaction_distribution['like'].sum()\n",
    "        total_wows = reaction_distribution['wow'].sum()\n",
    "        total_cares = reaction_distribution['cares'].sum()\n",
    "        total_sads = reaction_distribution['sad'].sum()\n",
    "        total_angrys = reaction_distribution['angry'].sum()\n",
    "        total_hahas = reaction_distribution['haha'].sum()\n",
    "\n",
    "        # Print the reaction distribution per news source\n",
    "        print(\"Reaction Distribution per News Source:\")\n",
    "        print(reaction_distribution)\n",
    "        print()\n",
    "\n",
    "        # Print the total likes, total wows, etc.\n",
    "        print('Total Likes:', total_likes)\n",
    "        print('Total Wows:', total_wows)\n",
    "        print('Total Cares:', total_cares)\n",
    "        print('Total Sads:', total_sads)\n",
    "        print('Total Angrys:', total_angrys)\n",
    "        print('Total Hahas:', total_hahas)\n",
    "        print()\n",
    "\n",
    "        # Calculate the total reactions for each reaction type\n",
    "        total_reactions = reaction_distribution.sum()\n",
    "\n",
    "        # Calculate the percentage of each reaction type of the total reactions\n",
    "        reaction_distribution['like_percentage'] = (reaction_distribution['like'] / total_reactions['like']) * 100\n",
    "        reaction_distribution['wow_percentage'] = (reaction_distribution['wow'] / total_reactions['wow']) * 100\n",
    "        reaction_distribution['cares_percentage'] = (reaction_distribution['cares'] / total_reactions['cares']) * 100\n",
    "        reaction_distribution['sad_percentage'] = (reaction_distribution['sad'] / total_reactions['sad']) * 100\n",
    "        reaction_distribution['angry_percentage'] = (reaction_distribution['angry'] / total_reactions['angry']) * 100\n",
    "        reaction_distribution['haha_percentage'] = (reaction_distribution['haha'] / total_reactions['haha']) * 100\n",
    "\n",
    "        # Print the total reactions\n",
    "        print(\"Total Reactions:\")\n",
    "        print(total_reactions)\n",
    "        print()\n",
    "\n",
    "        # Print the percentage of each reaction type of the total reactions\n",
    "        print('Like Percentage:', (total_reactions['like'] / reaction_distribution['total'].sum()) * 100)\n",
    "        print('Wow Percentage:', (total_reactions['wow'] / reaction_distribution['total'].sum()) * 100)\n",
    "        print('Cares Percentage:', (total_reactions['cares'] / reaction_distribution['total'].sum()) * 100)\n",
    "        print('Sad Percentage:', (total_reactions['sad'] / reaction_distribution['total'].sum()) * 100)\n",
    "        print('Angry Percentage:', (total_reactions['angry'] / reaction_distribution['total'].sum()) * 100)\n",
    "        print('Haha Percentage:', (total_reactions['haha'] / reaction_distribution['total'].sum()) * 100)\n",
    "\n",
    "        # Return the input data unmodified\n",
    "        return X\n",
    "\n",
    "class TextCleaner(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, stem=True, lemma=True, remove_special=True):\n",
    "        self.stem = stem\n",
    "        self.lemma = lemma\n",
    "        self.remove_special = remove_special\n",
    "        self.stop_words = set(stopwords.words('dutch'))\n",
    "        self.stemmer = SnowballStemmer('dutch')\n",
    "        self.lemmatizer = WordNetLemmatizer()\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        X.dropna(subset=['content'], inplace=True)\n",
    "        \n",
    "        # Remove special characters and numbers\n",
    "        if self.remove_special:\n",
    "            X['prep_content'] = X['content'].apply(lambda text: self._remove_special_chars_numbers(text))\n",
    "        \n",
    "        # Convert text to lowercase\n",
    "        X['prep_content'] = X['content'].apply(lambda text: text.lower())\n",
    "        \n",
    "        # Tokenize text\n",
    "        X['prep_content'] = X['content'].apply(lambda text: word_tokenize(text))\n",
    "        \n",
    "        # Remove stop words\n",
    "        X['prep_content'] = X['prep_content'].apply(lambda tokens: [token for token in tokens if not token in self.stop_words])\n",
    "        \n",
    "        # Apply stemming\n",
    "        if self.stem:\n",
    "            X['prep_content'] = X['prep_content'].apply(lambda tokens: [self.stemmer.stem(token) for token in tokens])\n",
    "        \n",
    "        # Apply lemmatization\n",
    "        if self.lemma:\n",
    "            X['prep_content'] = X['prep_content'].apply(lambda tokens: [self.lemmatizer.lemmatize(token) for token in tokens])\n",
    "        \n",
    "        # Join tokens back into text\n",
    "        X['prep_content'] = X['prep_content'].apply(lambda tokens: \" \".join(tokens))\n",
    "        \n",
    "        return X\n",
    "    \n",
    "    def _remove_special_chars_numbers(self, text):\n",
    "        # Remove special characters and numbers using string.punctuation\n",
    "        text = text.translate(str.maketrans(\"\", \"\", string.punctuation + string.digits))\n",
    "        return text\n",
    "\n",
    "    \n",
    "# Create the pipeline\n",
    "pipeline = Pipeline([\n",
    "    # Comment if you want to read in a csv file separately\n",
    "    ('csv_combiner', CSVCombiner(folder_path='INPUT FOLDER PATH')),\n",
    "    ('Append_Columns', ColumnAdder(col1='likes', col2='loves', new_col_name='like')),\n",
    "    ('entropy_calculator', EntropyCalculator(reaction_columns, total_reactions)),\n",
    "    ('word_cloud_generator', WordCloudGenerator()),\n",
    "    ('length_analysis', LengthAnalysisTransformer()),\n",
    "    ('descriptive_stats', DescriptiveStatistics()),\n",
    "    ('descriptive_stats_plotter', DescriptiveStatsPlotter()),\n",
    "    ('entropy_analyzer', EntropyAnalyzer()),\n",
    "    ('Append Source name',SourceAppender() ),\n",
    "    ('entropy_score_plotter', EntropyScorePlotter()),  # Add this step\n",
    "    ('reaction_analysis', ReactionAnalysisTransformer()),  # Add this step\n",
    "    ('Preprocess', PreprocessTransformer()),\n",
    "    ('TextCleaner', TextCleaner(stem=True, lemma=True, remove_special=True)),\n",
    "    # Comment if you do not want to write to csv\n",
    "    ('csv_writer', CSVWriter(file_path='INPUT FILE PATH')),\n",
    "\n",
    "])\n",
    "\n",
    "# Apply the pipeline to your DataFrame\n",
    "df_transformed = pipeline.fit_transform(df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "modelenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
