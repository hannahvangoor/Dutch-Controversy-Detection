{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/Hannah/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /Users/Hannah/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import timeit\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.stats import entropy\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import os\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.optimizers import Adam\n",
    "import tensorflow as tf\n",
    "\n",
    "import xgboost as xgb\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.svm import LinearSVR\n",
    "\n",
    "# Clean and tokenize text\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import string\n",
    "\n",
    "nltk.download('stopwords') # download stopwords corpus\n",
    "nltk.download('punkt') # download punkt tokenizer"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XGBOOST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation MSE: 0.11319878836264752\n",
      "Fitting 5 folds for each of 27 candidates, totalling 135 fits\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Best parameters: {'learning_rate': 0.1, 'max_depth': 3, 'n_estimators': 150}\n",
      "Test MSE (tuned): 0.099076315465922\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('transformers_dataNew.csv')\n",
    "df = df[df['reactions_count'] > 30]\n",
    "X = df.iloc[:, -768:-1]\n",
    "#X = df['prep_content']\n",
    "y = df['entropy']\n",
    "\n",
    "# In the first step we will split the data in training and remaining dataset\n",
    "\n",
    "X_train, X_rem, y_train, y_rem = train_test_split(X , y, train_size=0.8)\n",
    "\n",
    "# Now since we want the valid and test size to be equal (10% each of overall data). \n",
    "# we have to define valid_size=0.5 (that is 50% of remaining data)\n",
    "\n",
    "X_valid, X_test, y_valid, y_test = train_test_split(X_rem,y_rem, test_size=0.5)\n",
    "\n",
    "# create a Random Forest regressor object\n",
    "# Define XGBoost model\n",
    "XBBmodel = XGBRegressor(random_state = 42 )\n",
    "XBBmodel.fit(X_train, y_train)\n",
    "\n",
    "# make predictions on validation set\n",
    "y_val_pred = XBBmodel.predict(X_valid)\n",
    "\n",
    "# evaluate performance on validation set\n",
    "val_mse = mean_squared_error(y_valid, y_val_pred)\n",
    "print(\"Validation MSE:\", val_mse)\n",
    "\n",
    "# Define hyperparameter search space\n",
    "params = {\n",
    "    'max_depth': [3, 5, 7],\n",
    "    'n_estimators': [50, 100, 150],\n",
    "    'learning_rate': [0.1, 0.01, 0.001]\n",
    "}\n",
    "\n",
    "# Perform grid search with cross-validation\n",
    "grid_search = GridSearchCV(XBBmodel, param_grid=params, cv=5, n_jobs=-1, verbose=1)\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Print best hyperparameters\n",
    "print('Best parameters:', grid_search.best_params_)\n",
    "\n",
    "# Evaluate model on test set\n",
    "best_model = grid_search.best_estimator_\n",
    "y_pred = best_model.predict(X_test)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "print('Test MSE (tuned):', mse)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation MSE: 0.10925581161873402\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Best hyperparameters: {'max_depth': 15, 'min_samples_leaf': 4, 'min_samples_split': 2, 'n_estimators': 150}\n",
      "Test MSE (tuned): 0.10767828457147623\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('transformers_dataNew.csv')\n",
    "df = df[df['reactions_count'] > 30]\n",
    "X = df.iloc[:, -768:-1]\n",
    "#X = df['prep_content']\n",
    "y = df['entropy']\n",
    "\n",
    "# In the first step we will split the data in training and remaining dataset\n",
    "X_train, X_rem, y_train, y_rem = train_test_split(X , y, train_size=0.8)\n",
    "\n",
    "# Now since we want the valid and test size to be equal (10% each of overall data). \n",
    "# we have to define valid_size=0.5 (that is 50% of remaining data)\n",
    "X_valid, X_test, y_valid, y_test = train_test_split(X_rem,y_rem, test_size=0.5)\n",
    "\n",
    "# define the parameter grid to search\n",
    "param_grid = {\n",
    "    'n_estimators': [150],\n",
    "    'max_depth': [15],\n",
    "    'min_samples_split': [2],\n",
    "    'min_samples_leaf': [4]\n",
    "}\n",
    "\n",
    "# create a Random Forest regressor object\n",
    "rf_model = RandomForestRegressor(random_state=42)\n",
    "rf_model.fit(X_train, y_train)\n",
    "\n",
    "# make predictions on validation set\n",
    "y_val_pred = rf_model.predict(X_valid)\n",
    "\n",
    "# evaluate performance on validation set\n",
    "val_mse = mean_squared_error(y_valid, y_val_pred)\n",
    "print(\"Validation MSE:\", val_mse)\n",
    "\n",
    "# create a GridSearchCV object and fit it to the data\n",
    "rf_grid = GridSearchCV(rf_model, param_grid, cv=5, n_jobs=-1, scoring='neg_mean_squared_error')\n",
    "rf_grid.fit(X_train, y_train)\n",
    "\n",
    "# get the best model and print its hyperparameters\n",
    "best_rf = rf_grid.best_estimator_\n",
    "print(\"Best hyperparameters:\", rf_grid.best_params_)\n",
    "\n",
    "# evaluate the model on the test set\n",
    "y_pred = best_rf.predict(X_test)\n",
    "test_mse = mean_squared_error(y_pred, y_test)\n",
    "print(\"Test MSE (tuned):\", test_mse)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normal: \n",
    "Best hyperparameters: {'max_depth': 15, 'min_samples_leaf': 4, 'min_samples_split': 2, 'n_estimators': 150}\n",
    "Test MSE (tuned): 0.12172077754626903\n",
    "\n",
    "HIGH RC: Test MSE (tuned): 0.10767828457147623"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation MSE: 0.3023060723692162\n",
      "Best hyperparameters: {'C': 0.1, 'epsilon': 0.3}\n",
      "Test MSE (tuned): 0.11590169331844272\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('transformers_dataNew.csv')\n",
    "df = df[df['reactions_count'] > 30]\n",
    "X = df.iloc[:, -768:-1]\n",
    "#X = df['prep_content']\n",
    "y = df['entropy']\n",
    "\n",
    "# In the first step we will split the data in training and remaining dataset\n",
    "X_train, X_rem, y_train, y_rem = train_test_split(X , y, train_size=0.8)\n",
    "\n",
    "# Now since we want the valid and test size to be equal (10% each of overall data). \n",
    "# we have to define valid_size=0.5 (that is 50% of remaining data)\n",
    "X_valid, X_test, y_valid, y_test = train_test_split(X_rem,y_rem, test_size=0.5)\n",
    "\n",
    "# initialize and train the SVM regressor\n",
    "svm = LinearSVR(random_state=42)\n",
    "svm.fit(X_train, y_train)\n",
    "\n",
    "# make predictions on validation set\n",
    "y_val_pred = svm.predict(X_valid)\n",
    "\n",
    "# evaluate performance on validation set\n",
    "val_mse = mean_squared_error(y_valid, y_val_pred)\n",
    "print(\"Validation MSE:\", val_mse)\n",
    "\n",
    "# tune hyperparameters with GridSearchCV\n",
    "params = {\"C\": [0.1, 1, 10], \"epsilon\": [0.1, 0.2, 0.3]}\n",
    "svm_grid = GridSearchCV(svm, params, cv=5, scoring='neg_mean_squared_error')\n",
    "svm_grid.fit(X_valid, y_valid)\n",
    "\n",
    "# print best hyperparameters and corresponding performance on validation set\n",
    "print(\"Best hyperparameters:\", svm_grid.best_params_)\n",
    "best_svm = svm_grid.best_estimator_\n",
    "best_svm.fit(X_train, y_train)\n",
    "y_pred = best_svm.predict(X_test)\n",
    "test_mse = mean_squared_error(y_pred, y_test)\n",
    "print(\"Test MSE (tuned):\", test_mse)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NORMAL:\n",
    "Validation MSE: 0.13351362680688386\n",
    "Best hyperparameters: {'C': 0.1, 'epsilon': 0.3}\n",
    "Test MSE (tuned): 0.13003759970726186\n",
    "\n",
    "HIGH RC: Validation MSE: 0.3023060723692162\n",
    "Best hyperparameters: {'C': 0.1, 'epsilon': 0.3}\n",
    "Test MSE (tuned): 0.11590169331844272"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "modelenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
