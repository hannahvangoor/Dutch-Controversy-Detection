{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import time\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "import string\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.optimizers import Adam\n",
    "from tensorflow.python.ops.resource_variable_ops import var_handle_op\n",
    "from transformers import BertTokenizer, TFBertModel\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from tensorflow.keras import models, layers, preprocessing as kprocessing\n",
    "from tensorflow.keras import backend as K\n",
    "from transformers import AutoTokenizer, AutoModel, TFAutoModel\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "nltk.download('stopwords') # download stopwords corpus\n",
    "nltk.download('punkt') # download punkt tokenizer\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load the data\n",
    "df = pd.read_csv('') # INPUT DATA FILE\n",
    "X = df['content']\n",
    "y = df['entropy']\n",
    "\n",
    "# split train dataset into train, validation and test sets\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.3)\n",
    "\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5)\n",
    "\n",
    "# Plot the distribution of entropy scores in the test set\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.hist(y_train, bins=20, edgecolor='black', color='grey', alpha=0.75)\n",
    "plt.xlabel('Entropy')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Language model: Distribution of Entropy Scores in Train Set')\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.hist(y_test, bins=20, edgecolor='black', color='grey', alpha=0.75)\n",
    "plt.xlabel('Entropy')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Language model: Distribution of Entropy Scores in Test Set')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# BERT tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"GroNLP/bert-base-dutch-cased\")\n",
    "\n",
    "# Tokenize and encode the text data\n",
    "train_tokens = tokenizer.batch_encode_plus(\n",
    "    X_train.values.tolist(),\n",
    "    truncation=True,\n",
    "    padding='max_length',\n",
    "    max_length=128,\n",
    "    return_tensors='tf'\n",
    ")\n",
    "\n",
    "# Tokenize and encode the text data\n",
    "val_tokens = tokenizer.batch_encode_plus(\n",
    "    X_val.values.tolist(),\n",
    "    truncation=True,\n",
    "    padding='max_length',\n",
    "    max_length=128,\n",
    "    return_tensors='tf'\n",
    ")\n",
    "\n",
    "test_tokens = tokenizer.batch_encode_plus(\n",
    "    X_test.values.tolist(),\n",
    "    truncation=True,\n",
    "    padding='max_length',\n",
    "    max_length=128,\n",
    "    return_tensors='tf'\n",
    ")\n",
    "\n",
    "# Convert the tokenized input to TensorFlow Dataset\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((\n",
    "    dict(train_tokens),\n",
    "    y_train.values\n",
    "))\n",
    "\n",
    "val_dataset = tf.data.Dataset.from_tensor_slices((\n",
    "    dict(val_tokens),\n",
    "    y_val.values\n",
    "))\n",
    "\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices((\n",
    "    dict(test_tokens),\n",
    "    y_test.values\n",
    "))\n",
    "\n",
    "# Prepare the model input\n",
    "input_ids = tf.keras.layers.Input(shape=(128,), dtype=tf.int32, name='input_ids')\n",
    "attention_mask = tf.keras.layers.Input(shape=(128,), dtype=tf.int32, name='attention_mask')\n",
    "\n",
    "# Load BERT model\n",
    "model = TFAutoModel.from_pretrained(\"GroNLP/bert-base-dutch-cased\")  # Tensorflow\n",
    "\n",
    "# Freeze BERT layers\n",
    "model.trainable = False\n",
    "\n",
    "# Retrieve the BERT embeddings\n",
    "embeddings = model(input_ids,attention_mask)[0]\n",
    "\n",
    "# Perform pooling (average pooling in this case)\n",
    "pooled_output = tf.keras.layers.GlobalAveragePooling1D()(embeddings)\n",
    "pooled_output = tf.keras.layers.Dropout(0.1)(pooled_output)\n",
    "pooled_output = tf.keras.layers.Dense(32, activation='relu')(pooled_output)\n",
    "pooled_output = tf.keras.layers.Dropout(0.1)(pooled_output)\n",
    "\n",
    "# Dense layer for regression output\n",
    "output = tf.keras.layers.Dense(1, activation='linear')(pooled_output)\n",
    "\n",
    "# Build the model\n",
    "model = tf.keras.Model(inputs=[input_ids, attention_mask], outputs=output)\n",
    "\n",
    "# Compile the model, learning rate: 1e-4\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=5e-4), loss='mean_squared_error')\n",
    "\n",
    "model.summary()\n",
    "\n",
    "training = model.fit(train_dataset.batch(256), epochs=10, shuffle=True, verbose=0, validation_data=val_dataset.batch(256))\n",
    "\n",
    "# Evaluate the best model on the test set\n",
    "predictions = model.predict(test_dataset.batch(32))\n",
    "mse = mean_squared_error(y_test.values, predictions)\n",
    "print('Best Model: Mean Squared Error:', mse)\n",
    "#print('Best Hyperparameters:', best_params)\n",
    "\n",
    "# Print the content, entropy, and predicted entropy (examples)\n",
    "for content, entropy, pred in zip(X_test, y_test.values, predictions.flatten()):\n",
    "    print(f\"Content: {content}\")\n",
    "    print(f\"Entropy: {entropy}\")\n",
    "    print(f\"Predicted Entropy: {pred}\")\n",
    "    print()\n",
    "\n",
    "# Calculate the errors\n",
    "errors = y_test.values - predictions.flatten()\n",
    "\n",
    "# Calculate the variance and standard deviation of errors\n",
    "var = np.var(errors)\n",
    "std = np.std(errors)\n",
    "print('Language Model: Variance of Errors:', var)\n",
    "print('Language Model: Standard Deviation of Errors:', std)\n",
    "\n",
    "\n",
    "# Plot the distribution of errors\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.hist(errors, bins=20, edgecolor='black', alpha=0.75)\n",
    "plt.xlabel('Error')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Language Model: Distribution of Errors in Test Set')\n",
    "plt.show()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Resources:\n",
    " https://towardsdatascience.com/text-classification-with-nlp-tf-idf-vs-word2vec-vs-bert-41ff868d1794\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "modelenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
