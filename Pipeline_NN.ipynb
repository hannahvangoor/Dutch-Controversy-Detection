{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/Hannah/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /Users/Hannah/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import time\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "import string\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.optimizers import Adam\n",
    "from tensorflow.python.ops.resource_variable_ops import var_handle_op\n",
    "from transformers import BertTokenizer, TFBertModel\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from tensorflow.keras import models, layers, preprocessing as kprocessing\n",
    "from tensorflow.keras import backend as K\n",
    "from transformers import AutoTokenizer, AutoModel, TFAutoModel\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "nltk.download('stopwords') # download stopwords corpus\n",
    "nltk.download('punkt') # download punkt tokenizer\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/Hannah/Documents/VU/Msc/Thesis/Coding/Pipeline\n"
     ]
    }
   ],
   "source": [
    "%cd /Users/Hannah/Documents/VU/Msc/Thesis/Coding/Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at GroNLP/bert-base-dutch-cased were not used when initializing TFBertModel: ['mlm___cls']\n",
      "- This IS expected if you are initializing TFBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some layers of TFBertModel were not initialized from the model checkpoint at GroNLP/bert-base-dutch-cased and are newly initialized: ['bert/pooler/dense/kernel:0', 'bert/pooler/dense/bias:0']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_4\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_ids (InputLayer)         [(None, 128)]        0           []                               \n",
      "                                                                                                  \n",
      " attention_mask (InputLayer)    [(None, 128)]        0           []                               \n",
      "                                                                                                  \n",
      " tf_bert_model_4 (TFBertModel)  TFBaseModelOutputWi  109137408   ['input_ids[0][0]',              \n",
      "                                thPoolingAndCrossAt               'attention_mask[0][0]']         \n",
      "                                tentions(last_hidde                                               \n",
      "                                n_state=(None, 128,                                               \n",
      "                                 768),                                                            \n",
      "                                 pooler_output=(Non                                               \n",
      "                                e, 768),                                                          \n",
      "                                 past_key_values=No                                               \n",
      "                                ne, hidden_states=N                                               \n",
      "                                one, attentions=Non                                               \n",
      "                                e, cross_attentions                                               \n",
      "                                =None)                                                            \n",
      "                                                                                                  \n",
      " global_average_pooling1d_4 (Gl  (None, 768)         0           ['tf_bert_model_4[0][0]']        \n",
      " obalAveragePooling1D)                                                                            \n",
      "                                                                                                  \n",
      " dropout_193 (Dropout)          (None, 768)          0           ['global_average_pooling1d_4[0][0\n",
      "                                                                 ]']                              \n",
      "                                                                                                  \n",
      " dense_8 (Dense)                (None, 64)           49216       ['dropout_193[0][0]']            \n",
      "                                                                                                  \n",
      " dropout_194 (Dropout)          (None, 64)           0           ['dense_8[0][0]']                \n",
      "                                                                                                  \n",
      " dense_9 (Dense)                (None, 1)            65          ['dropout_194[0][0]']            \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 109,186,689\n",
      "Trainable params: 49,281\n",
      "Non-trainable params: 109,137,408\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-11 20:19:17.527289: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_3' with dtype double and shape [4629]\n",
      "\t [[{{node Placeholder/_3}}]]\n",
      "/opt/anaconda3/envs/modelenv/lib/python3.11/site-packages/keras/engine/functional.py:639: UserWarning: Input dict contained keys ['token_type_ids'] which did not match any model input. They will be ignored by the model.\n",
      "  inputs = self._flatten_to_reference_inputs(inputs)\n",
      "2023-06-11 20:34:02.592711: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_3' with dtype double and shape [992]\n",
      "\t [[{{node Placeholder/_3}}]]\n",
      "/var/folders/24/zyx78_1d5fndpjnbr8x7dd6r0000gn/T/ipykernel_6649/2056651634.py:96: DeprecationWarning: KerasRegressor is deprecated, use Sci-Keras (https://github.com/adriangb/scikeras) instead. See https://www.adriangb.com/scikeras/stable/migration.html for help migrating.\n",
      "  keras_regressor = tf.keras.wrappers.scikit_learn.KerasRegressor(model)\n",
      "2023-06-11 22:59:58.612829: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_3' with dtype double and shape [4629]\n",
      "\t [[{{node Placeholder/_3}}]]\n",
      "/var/folders/24/zyx78_1d5fndpjnbr8x7dd6r0000gn/T/ipykernel_6649/2056651634.py:99: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  train_data = np.array(list(train_dataset.batch(256).as_numpy_iterator()))\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Found input variables with inconsistent numbers of samples: [19, 4629]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 106\u001b[0m\n\u001b[1;32m    103\u001b[0m grid_search \u001b[39m=\u001b[39m GridSearchCV(estimator\u001b[39m=\u001b[39mkeras_regressor, param_grid\u001b[39m=\u001b[39mparam_grid, cv\u001b[39m=\u001b[39m\u001b[39m3\u001b[39m)\n\u001b[1;32m    105\u001b[0m \u001b[39m# Fit the grid search on the training data\u001b[39;00m\n\u001b[0;32m--> 106\u001b[0m grid_search\u001b[39m.\u001b[39mfit(train_data, y_train_data)\n\u001b[1;32m    108\u001b[0m \u001b[39m# Fit the grid search on the training data\u001b[39;00m\n\u001b[1;32m    109\u001b[0m \n\u001b[1;32m    110\u001b[0m \u001b[39m# Get the best model and its hyperparameters\u001b[39;00m\n\u001b[1;32m    111\u001b[0m best_model \u001b[39m=\u001b[39m grid_search\u001b[39m.\u001b[39mbest_estimator_\n",
      "File \u001b[0;32m/opt/anaconda3/envs/modelenv/lib/python3.11/site-packages/sklearn/model_selection/_search.py:782\u001b[0m, in \u001b[0;36mBaseSearchCV.fit\u001b[0;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[1;32m    779\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_check_refit_for_multimetric(scorers)\n\u001b[1;32m    780\u001b[0m     refit_metric \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrefit\n\u001b[0;32m--> 782\u001b[0m X, y, groups \u001b[39m=\u001b[39m indexable(X, y, groups)\n\u001b[1;32m    783\u001b[0m fit_params \u001b[39m=\u001b[39m _check_fit_params(X, fit_params)\n\u001b[1;32m    785\u001b[0m cv_orig \u001b[39m=\u001b[39m check_cv(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcv, y, classifier\u001b[39m=\u001b[39mis_classifier(estimator))\n",
      "File \u001b[0;32m/opt/anaconda3/envs/modelenv/lib/python3.11/site-packages/sklearn/utils/validation.py:443\u001b[0m, in \u001b[0;36mindexable\u001b[0;34m(*iterables)\u001b[0m\n\u001b[1;32m    424\u001b[0m \u001b[39m\"\"\"Make arrays indexable for cross-validation.\u001b[39;00m\n\u001b[1;32m    425\u001b[0m \n\u001b[1;32m    426\u001b[0m \u001b[39mChecks consistent length, passes through None, and ensures that everything\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    439\u001b[0m \u001b[39m    sparse matrix, or dataframe) or `None`.\u001b[39;00m\n\u001b[1;32m    440\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    442\u001b[0m result \u001b[39m=\u001b[39m [_make_indexable(X) \u001b[39mfor\u001b[39;00m X \u001b[39min\u001b[39;00m iterables]\n\u001b[0;32m--> 443\u001b[0m check_consistent_length(\u001b[39m*\u001b[39mresult)\n\u001b[1;32m    444\u001b[0m \u001b[39mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m/opt/anaconda3/envs/modelenv/lib/python3.11/site-packages/sklearn/utils/validation.py:397\u001b[0m, in \u001b[0;36mcheck_consistent_length\u001b[0;34m(*arrays)\u001b[0m\n\u001b[1;32m    395\u001b[0m uniques \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39munique(lengths)\n\u001b[1;32m    396\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(uniques) \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[0;32m--> 397\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    398\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mFound input variables with inconsistent numbers of samples: \u001b[39m\u001b[39m%r\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m    399\u001b[0m         \u001b[39m%\u001b[39m [\u001b[39mint\u001b[39m(l) \u001b[39mfor\u001b[39;00m l \u001b[39min\u001b[39;00m lengths]\n\u001b[1;32m    400\u001b[0m     )\n",
      "\u001b[0;31mValueError\u001b[0m: Found input variables with inconsistent numbers of samples: [19, 4629]"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Load the data\n",
    "df = pd.read_csv('Indicator-Desc_DataNewVanPipeline.csv')\n",
    "X = df['content']\n",
    "y = df['entropy']\n",
    "\n",
    "# split train dataset into train, validation and test sets\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.3)\n",
    "\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5)\n",
    "\n",
    "#X_train, X_test, y_train, y_test = train_test_split(X , y, train_size=0.8)\n",
    "\n",
    "# BERT tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"GroNLP/bert-base-dutch-cased\")\n",
    "\n",
    "# Tokenize and encode the text data\n",
    "train_tokens = tokenizer.batch_encode_plus(\n",
    "    X_train.values.tolist(),\n",
    "    truncation=True,\n",
    "    padding='max_length',\n",
    "    max_length=128,\n",
    "    return_tensors='tf'\n",
    ")\n",
    "\n",
    "# Tokenize and encode the text data\n",
    "val_tokens = tokenizer.batch_encode_plus(\n",
    "    X_val.values.tolist(),\n",
    "    truncation=True,\n",
    "    padding='max_length',\n",
    "    max_length=128,\n",
    "    return_tensors='tf'\n",
    ")\n",
    "\n",
    "test_tokens = tokenizer.batch_encode_plus(\n",
    "    X_test.values.tolist(),\n",
    "    truncation=True,\n",
    "    padding='max_length',\n",
    "    max_length=128,\n",
    "    return_tensors='tf'\n",
    ")\n",
    "\n",
    "# Convert the tokenized input to TensorFlow Dataset\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((\n",
    "    dict(train_tokens),\n",
    "    y_train.values\n",
    "))\n",
    "\n",
    "val_dataset = tf.data.Dataset.from_tensor_slices((\n",
    "    dict(val_tokens),\n",
    "    y_val.values\n",
    "))\n",
    "\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices((\n",
    "    dict(test_tokens),\n",
    "    y_test.values\n",
    "))\n",
    "\n",
    "# Prepare the model input\n",
    "input_ids = tf.keras.layers.Input(shape=(128,), dtype=tf.int32, name='input_ids')\n",
    "attention_mask = tf.keras.layers.Input(shape=(128,), dtype=tf.int32, name='attention_mask')\n",
    "\n",
    "# Load BERT model\n",
    "model = TFAutoModel.from_pretrained(\"GroNLP/bert-base-dutch-cased\")  # Tensorflow\n",
    "\n",
    "# Freeze BERT layers\n",
    "model.trainable = False\n",
    "\n",
    "# Retrieve the BERT embeddings\n",
    "embeddings = model(input_ids,attention_mask)[0]\n",
    "\n",
    "# Perform pooling (average pooling in this case)\n",
    "pooled_output = tf.keras.layers.GlobalAveragePooling1D()(embeddings)\n",
    "pooled_output = tf.keras.layers.Dropout(0.1)(pooled_output)\n",
    "pooled_output = tf.keras.layers.Dense(64, activation='relu')(pooled_output)\n",
    "pooled_output = tf.keras.layers.Dropout(0.1)(pooled_output)\n",
    "\n",
    "# Dense layer for regression output\n",
    "output = tf.keras.layers.Dense(1, activation='linear')(pooled_output)\n",
    "\n",
    "# Build the model\n",
    "model = tf.keras.Model(inputs=[input_ids, attention_mask], outputs=output)\n",
    "\n",
    "# Compile the model, learning rate: 1e-4\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=5e-4), loss='mean_squared_error')\n",
    "model.summary()\n",
    "training = model.fit(train_dataset.batch(256), epochs=10, shuffle=True, verbose=0, validation_data=val_dataset.batch(256))\n",
    "\n",
    "\n",
    "param_grid = {\n",
    "    'learning_rate': [1e-4, 5e-4],\n",
    "    'dropout_rate': [0.1, 0.2],\n",
    "    'epochs': [5, 10]\n",
    "}\n",
    "\n",
    "# Wrap the model with KerasRegressor for grid search\n",
    "keras_regressor = tf.keras.wrappers.scikit_learn.KerasRegressor(model)\n",
    "\n",
    "# Convert TensorFlow datasets to NumPy arrays\n",
    "train_data = np.array(list(train_dataset.batch(256).as_numpy_iterator()))\n",
    "y_train_data = np.array(list(y_train.values))\n",
    "\n",
    "# Perform grid search using GridSearchCV\n",
    "grid_search = GridSearchCV(estimator=keras_regressor, param_grid=param_grid, cv=3)\n",
    "\n",
    "# Fit the grid search on the training data\n",
    "grid_search.fit(train_data, y_train_data)\n",
    "\n",
    "# Fit the grid search on the training data\n",
    "\n",
    "# Get the best model and its hyperparameters\n",
    "best_model = grid_search.best_estimator_\n",
    "best_params = grid_search.best_params_\n",
    "\n",
    "# Evaluate the best model on the test set\n",
    "predictions = best_model.predict(test_dataset.batch(32))\n",
    "mse = mean_squared_error(y_test.values, predictions)\n",
    "print('Best Model: Mean Squared Error:', mse)\n",
    "print('Best Hyperparameters:', best_params)\n",
    "\n",
    "\n",
    "# Calculate the errors\n",
    "errors = y_test.values - predictions.flatten()\n",
    "\n",
    "# Calculate the variance and standard deviation of errors\n",
    "var = np.var(errors)\n",
    "std = np.std(errors)\n",
    "print('Language Model: Variance of Errors:', var)\n",
    "print('Language Model: Standard Deviation of Errors:', std)\n",
    "\n",
    "# Plot the distribution of errors\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.hist(errors, bins=20, edgecolor='black', alpha=0.75)\n",
    "plt.xlabel('Error')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Language Model: Distribution of Errors in Test Set')\n",
    "plt.show()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Resources:\n",
    " https://towardsdatascience.com/text-classification-with-nlp-tf-idf-vs-word2vec-vs-bert-41ff868d1794\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "modelenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
