{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-10 12:37:18.434516: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "/opt/anaconda3/envs/modelenv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/Hannah/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /Users/Hannah/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.optimizers import Adam\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "## for deep learning\n",
    "from tensorflow.keras import models, layers, preprocessing as kprocessing\n",
    "from tensorflow.keras import backend as K\n",
    "## for bert language model\n",
    "from transformers import AutoTokenizer, AutoModel, TFAutoModel\n",
    "import re\n",
    "\n",
    "# Clean and tokenize text\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import string\n",
    "\n",
    "nltk.download('stopwords') # download stopwords corpus\n",
    "nltk.download('punkt') # download punkt tokenizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/Hannah/Documents/VU/Msc/Thesis/Coding/Pipeline\n"
     ]
    }
   ],
   "source": [
    "%cd /Users/Hannah/Documents/VU/Msc/Thesis/Coding/Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('transformers_dataNew.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dropna(subset=['content'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>shares</th>\n",
       "      <th>wow</th>\n",
       "      <th>cares</th>\n",
       "      <th>sad</th>\n",
       "      <th>angry</th>\n",
       "      <th>haha</th>\n",
       "      <th>reactions_count</th>\n",
       "      <th>comments</th>\n",
       "      <th>content</th>\n",
       "      <th>...</th>\n",
       "      <th>embedding_759</th>\n",
       "      <th>embedding_760</th>\n",
       "      <th>embedding_761</th>\n",
       "      <th>embedding_762</th>\n",
       "      <th>embedding_763</th>\n",
       "      <th>embedding_764</th>\n",
       "      <th>embedding_765</th>\n",
       "      <th>embedding_766</th>\n",
       "      <th>embedding_767</th>\n",
       "      <th>embeddings</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>RTL Nieuws</td>\n",
       "      <td>0</td>\n",
       "      <td>17</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>14</td>\n",
       "      <td>39</td>\n",
       "      <td>0</td>\n",
       "      <td>Medewerkers van een aquarium in Spanje aten ge...</td>\n",
       "      <td>...</td>\n",
       "      <td>13.776934</td>\n",
       "      <td>0.816872</td>\n",
       "      <td>0.773461</td>\n",
       "      <td>1.520865</td>\n",
       "      <td>1.432767</td>\n",
       "      <td>1.400064</td>\n",
       "      <td>0.703925</td>\n",
       "      <td>0.769034</td>\n",
       "      <td>1.060223</td>\n",
       "      <td>[ 2.12260270e+00  1.95452666e+00  6.88976407e-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>RTL Nieuws</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>15</td>\n",
       "      <td>0</td>\n",
       "      <td>\"Er wordt wiet gerookt in het boek, gezoend, e...</td>\n",
       "      <td>...</td>\n",
       "      <td>13.530748</td>\n",
       "      <td>1.590019</td>\n",
       "      <td>1.094324</td>\n",
       "      <td>1.871268</td>\n",
       "      <td>1.652691</td>\n",
       "      <td>1.231773</td>\n",
       "      <td>1.189072</td>\n",
       "      <td>1.068386</td>\n",
       "      <td>1.233858</td>\n",
       "      <td>[ 2.13255215e+00  1.18901312e+00  7.16963530e-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>RTL Nieuws</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>38</td>\n",
       "      <td>67</td>\n",
       "      <td>206</td>\n",
       "      <td>Andy en het team van de dragbar laten zich, on...</td>\n",
       "      <td>...</td>\n",
       "      <td>13.372700</td>\n",
       "      <td>1.080609</td>\n",
       "      <td>0.964037</td>\n",
       "      <td>1.063937</td>\n",
       "      <td>1.858110</td>\n",
       "      <td>1.554261</td>\n",
       "      <td>0.934504</td>\n",
       "      <td>0.737435</td>\n",
       "      <td>1.404058</td>\n",
       "      <td>[ 2.12687016e+00  1.41113973e+00  9.98041630e-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>RTL Nieuws</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>7</td>\n",
       "      <td>19</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>30</td>\n",
       "      <td>26</td>\n",
       "      <td>Onder de slachtoffers waren volgens VN-mensenr...</td>\n",
       "      <td>...</td>\n",
       "      <td>13.289650</td>\n",
       "      <td>1.249961</td>\n",
       "      <td>1.254739</td>\n",
       "      <td>1.537239</td>\n",
       "      <td>1.924307</td>\n",
       "      <td>1.640161</td>\n",
       "      <td>0.366889</td>\n",
       "      <td>0.969132</td>\n",
       "      <td>0.716310</td>\n",
       "      <td>[ 2.9026854   0.780339    1.11986625  1.063458...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>RTL Nieuws</td>\n",
       "      <td>5</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>21</td>\n",
       "      <td>58</td>\n",
       "      <td>284</td>\n",
       "      <td>Alleen al de afgelopen drie dagen kwamen meer ...</td>\n",
       "      <td>...</td>\n",
       "      <td>13.318510</td>\n",
       "      <td>1.263664</td>\n",
       "      <td>0.760335</td>\n",
       "      <td>1.193509</td>\n",
       "      <td>1.659033</td>\n",
       "      <td>1.697025</td>\n",
       "      <td>1.157894</td>\n",
       "      <td>1.496184</td>\n",
       "      <td>1.604272</td>\n",
       "      <td>[ 1.75874364  0.47566167  1.14991474  0.760251...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9995</th>\n",
       "      <td>De Telegraaf</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>47</td>\n",
       "      <td>103</td>\n",
       "      <td>169</td>\n",
       "      <td>Elke aardbewoner surft jaarlijks gemiddeld mee...</td>\n",
       "      <td>...</td>\n",
       "      <td>13.060578</td>\n",
       "      <td>1.298744</td>\n",
       "      <td>0.832913</td>\n",
       "      <td>1.100882</td>\n",
       "      <td>1.712117</td>\n",
       "      <td>0.040602</td>\n",
       "      <td>0.658332</td>\n",
       "      <td>1.102006</td>\n",
       "      <td>1.012051</td>\n",
       "      <td>[ 2.00863171  1.04274106  0.94798875  0.684139...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9996</th>\n",
       "      <td>De Telegraaf</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>17</td>\n",
       "      <td>60</td>\n",
       "      <td>26</td>\n",
       "      <td>De Nederlander en zijn team Red Bull hadden de...</td>\n",
       "      <td>...</td>\n",
       "      <td>13.239983</td>\n",
       "      <td>0.822294</td>\n",
       "      <td>0.695602</td>\n",
       "      <td>1.593539</td>\n",
       "      <td>1.619219</td>\n",
       "      <td>1.291816</td>\n",
       "      <td>0.724189</td>\n",
       "      <td>0.918574</td>\n",
       "      <td>1.635511</td>\n",
       "      <td>[ 1.101861    1.73083413  0.7798183   0.473033...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9997</th>\n",
       "      <td>De Telegraaf</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>23</td>\n",
       "      <td>39</td>\n",
       "      <td>71</td>\n",
       "      <td>65</td>\n",
       "      <td>„We weten weinig van de mogelijke effecten van...</td>\n",
       "      <td>...</td>\n",
       "      <td>12.788166</td>\n",
       "      <td>1.192427</td>\n",
       "      <td>0.812722</td>\n",
       "      <td>1.040017</td>\n",
       "      <td>1.249091</td>\n",
       "      <td>1.165109</td>\n",
       "      <td>0.630857</td>\n",
       "      <td>1.169717</td>\n",
       "      <td>1.338238</td>\n",
       "      <td>[ 2.35046268  1.5198431   0.71057779  1.275846...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9998</th>\n",
       "      <td>De Telegraaf</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>60</td>\n",
       "      <td>91</td>\n",
       "      <td>135</td>\n",
       "      <td>Het rommelt binnen het CDA. De Telegraaf</td>\n",
       "      <td>...</td>\n",
       "      <td>12.757520</td>\n",
       "      <td>0.974428</td>\n",
       "      <td>0.540617</td>\n",
       "      <td>1.341013</td>\n",
       "      <td>1.987473</td>\n",
       "      <td>0.697544</td>\n",
       "      <td>0.471119</td>\n",
       "      <td>0.873903</td>\n",
       "      <td>1.283651</td>\n",
       "      <td>[ 2.13793945e+00  1.02813029e+00  3.28069597e-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9999</th>\n",
       "      <td>De Telegraaf</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>32</td>\n",
       "      <td>0</td>\n",
       "      <td>21</td>\n",
       "      <td>115</td>\n",
       "      <td>0</td>\n",
       "      <td>Balen! De Telegraaf</td>\n",
       "      <td>...</td>\n",
       "      <td>13.192951</td>\n",
       "      <td>1.297148</td>\n",
       "      <td>1.024886</td>\n",
       "      <td>1.223937</td>\n",
       "      <td>0.638661</td>\n",
       "      <td>1.344255</td>\n",
       "      <td>0.668650</td>\n",
       "      <td>0.120446</td>\n",
       "      <td>0.707811</td>\n",
       "      <td>[ 1.05763030e+00  1.12195158e+00  6.78173304e-...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>9975 rows × 789 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              name  shares  wow  cares  sad  angry  haha  reactions_count   \n",
       "0       RTL Nieuws       0   17      0    0      0    14               39  \\\n",
       "1       RTL Nieuws       0    1      0    0      0    10               15   \n",
       "2       RTL Nieuws       1    0     11    0      0    38               67   \n",
       "3       RTL Nieuws       2    4      7   19      0     0               30   \n",
       "4       RTL Nieuws       5   12      0    0      0    21               58   \n",
       "...            ...     ...  ...    ...  ...    ...   ...              ...   \n",
       "9995  De Telegraaf       3    0      0    0      0    47              103   \n",
       "9996  De Telegraaf       4    0      0    0      0    17               60   \n",
       "9997  De Telegraaf       2    0      0    0     23    39               71   \n",
       "9998  De Telegraaf       3    0      0    0      0    60               91   \n",
       "9999  De Telegraaf       0    0      0   32      0    21              115   \n",
       "\n",
       "      comments                                            content  ...   \n",
       "0            0  Medewerkers van een aquarium in Spanje aten ge...  ...  \\\n",
       "1            0  \"Er wordt wiet gerookt in het boek, gezoend, e...  ...   \n",
       "2          206  Andy en het team van de dragbar laten zich, on...  ...   \n",
       "3           26  Onder de slachtoffers waren volgens VN-mensenr...  ...   \n",
       "4          284  Alleen al de afgelopen drie dagen kwamen meer ...  ...   \n",
       "...        ...                                                ...  ...   \n",
       "9995       169  Elke aardbewoner surft jaarlijks gemiddeld mee...  ...   \n",
       "9996        26  De Nederlander en zijn team Red Bull hadden de...  ...   \n",
       "9997        65  „We weten weinig van de mogelijke effecten van...  ...   \n",
       "9998       135           Het rommelt binnen het CDA. De Telegraaf  ...   \n",
       "9999         0                                Balen! De Telegraaf  ...   \n",
       "\n",
       "     embedding_759 embedding_760  embedding_761  embedding_762  embedding_763   \n",
       "0        13.776934      0.816872       0.773461       1.520865       1.432767  \\\n",
       "1        13.530748      1.590019       1.094324       1.871268       1.652691   \n",
       "2        13.372700      1.080609       0.964037       1.063937       1.858110   \n",
       "3        13.289650      1.249961       1.254739       1.537239       1.924307   \n",
       "4        13.318510      1.263664       0.760335       1.193509       1.659033   \n",
       "...            ...           ...            ...            ...            ...   \n",
       "9995     13.060578      1.298744       0.832913       1.100882       1.712117   \n",
       "9996     13.239983      0.822294       0.695602       1.593539       1.619219   \n",
       "9997     12.788166      1.192427       0.812722       1.040017       1.249091   \n",
       "9998     12.757520      0.974428       0.540617       1.341013       1.987473   \n",
       "9999     13.192951      1.297148       1.024886       1.223937       0.638661   \n",
       "\n",
       "      embedding_764  embedding_765  embedding_766  embedding_767   \n",
       "0          1.400064       0.703925       0.769034       1.060223  \\\n",
       "1          1.231773       1.189072       1.068386       1.233858   \n",
       "2          1.554261       0.934504       0.737435       1.404058   \n",
       "3          1.640161       0.366889       0.969132       0.716310   \n",
       "4          1.697025       1.157894       1.496184       1.604272   \n",
       "...             ...            ...            ...            ...   \n",
       "9995       0.040602       0.658332       1.102006       1.012051   \n",
       "9996       1.291816       0.724189       0.918574       1.635511   \n",
       "9997       1.165109       0.630857       1.169717       1.338238   \n",
       "9998       0.697544       0.471119       0.873903       1.283651   \n",
       "9999       1.344255       0.668650       0.120446       0.707811   \n",
       "\n",
       "                                             embeddings  \n",
       "0     [ 2.12260270e+00  1.95452666e+00  6.88976407e-...  \n",
       "1     [ 2.13255215e+00  1.18901312e+00  7.16963530e-...  \n",
       "2     [ 2.12687016e+00  1.41113973e+00  9.98041630e-...  \n",
       "3     [ 2.9026854   0.780339    1.11986625  1.063458...  \n",
       "4     [ 1.75874364  0.47566167  1.14991474  0.760251...  \n",
       "...                                                 ...  \n",
       "9995  [ 2.00863171  1.04274106  0.94798875  0.684139...  \n",
       "9996  [ 1.101861    1.73083413  0.7798183   0.473033...  \n",
       "9997  [ 2.35046268  1.5198431   0.71057779  1.275846...  \n",
       "9998  [ 2.13793945e+00  1.02813029e+00  3.28069597e-...  \n",
       "9999  [ 1.05763030e+00  1.12195158e+00  6.78173304e-...  \n",
       "\n",
       "[9975 rows x 789 columns]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "TextEncodeInput must be Union[TextInputSequence, Tuple[InputSequence, InputSequence]]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[63], line 20\u001b[0m\n\u001b[1;32m     17\u001b[0m tokenizer \u001b[39m=\u001b[39m AutoTokenizer\u001b[39m.\u001b[39mfrom_pretrained(\u001b[39m\"\u001b[39m\u001b[39mGroNLP/bert-base-dutch-cased\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     19\u001b[0m \u001b[39m# Tokenize and encode the text data\u001b[39;00m\n\u001b[0;32m---> 20\u001b[0m train_tokens \u001b[39m=\u001b[39m tokenizer\u001b[39m.\u001b[39mbatch_encode_plus(\n\u001b[1;32m     21\u001b[0m     train_data[\u001b[39m'\u001b[39m\u001b[39mcontent\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mtolist(),\n\u001b[1;32m     22\u001b[0m     truncation\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m,\n\u001b[1;32m     23\u001b[0m     padding\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mmax_length\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[1;32m     24\u001b[0m     max_length\u001b[39m=\u001b[39m\u001b[39m128\u001b[39m,\n\u001b[1;32m     25\u001b[0m     return_tensors\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mtf\u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m     26\u001b[0m )\n\u001b[1;32m     28\u001b[0m valid_tokens \u001b[39m=\u001b[39m tokenizer\u001b[39m.\u001b[39mbatch_encode_plus(\n\u001b[1;32m     29\u001b[0m     valid_data[\u001b[39m'\u001b[39m\u001b[39mcontent\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mtolist(),\n\u001b[1;32m     30\u001b[0m     truncation\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     33\u001b[0m     return_tensors\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mtf\u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m     34\u001b[0m )\n\u001b[1;32m     36\u001b[0m test_tokens \u001b[39m=\u001b[39m tokenizer\u001b[39m.\u001b[39mbatch_encode_plus(\n\u001b[1;32m     37\u001b[0m     test_data[\u001b[39m'\u001b[39m\u001b[39mcontent\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mtolist(),\n\u001b[1;32m     38\u001b[0m     truncation\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     41\u001b[0m     return_tensors\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mtf\u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m     42\u001b[0m )\n",
      "File \u001b[0;32m/opt/anaconda3/envs/modelenv/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:2815\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.batch_encode_plus\u001b[0;34m(self, batch_text_or_text_pairs, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[1;32m   2805\u001b[0m \u001b[39m# Backward compatibility for 'truncation_strategy', 'pad_to_max_length'\u001b[39;00m\n\u001b[1;32m   2806\u001b[0m padding_strategy, truncation_strategy, max_length, kwargs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_padding_truncation_strategies(\n\u001b[1;32m   2807\u001b[0m     padding\u001b[39m=\u001b[39mpadding,\n\u001b[1;32m   2808\u001b[0m     truncation\u001b[39m=\u001b[39mtruncation,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2812\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs,\n\u001b[1;32m   2813\u001b[0m )\n\u001b[0;32m-> 2815\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_batch_encode_plus(\n\u001b[1;32m   2816\u001b[0m     batch_text_or_text_pairs\u001b[39m=\u001b[39mbatch_text_or_text_pairs,\n\u001b[1;32m   2817\u001b[0m     add_special_tokens\u001b[39m=\u001b[39madd_special_tokens,\n\u001b[1;32m   2818\u001b[0m     padding_strategy\u001b[39m=\u001b[39mpadding_strategy,\n\u001b[1;32m   2819\u001b[0m     truncation_strategy\u001b[39m=\u001b[39mtruncation_strategy,\n\u001b[1;32m   2820\u001b[0m     max_length\u001b[39m=\u001b[39mmax_length,\n\u001b[1;32m   2821\u001b[0m     stride\u001b[39m=\u001b[39mstride,\n\u001b[1;32m   2822\u001b[0m     is_split_into_words\u001b[39m=\u001b[39mis_split_into_words,\n\u001b[1;32m   2823\u001b[0m     pad_to_multiple_of\u001b[39m=\u001b[39mpad_to_multiple_of,\n\u001b[1;32m   2824\u001b[0m     return_tensors\u001b[39m=\u001b[39mreturn_tensors,\n\u001b[1;32m   2825\u001b[0m     return_token_type_ids\u001b[39m=\u001b[39mreturn_token_type_ids,\n\u001b[1;32m   2826\u001b[0m     return_attention_mask\u001b[39m=\u001b[39mreturn_attention_mask,\n\u001b[1;32m   2827\u001b[0m     return_overflowing_tokens\u001b[39m=\u001b[39mreturn_overflowing_tokens,\n\u001b[1;32m   2828\u001b[0m     return_special_tokens_mask\u001b[39m=\u001b[39mreturn_special_tokens_mask,\n\u001b[1;32m   2829\u001b[0m     return_offsets_mapping\u001b[39m=\u001b[39mreturn_offsets_mapping,\n\u001b[1;32m   2830\u001b[0m     return_length\u001b[39m=\u001b[39mreturn_length,\n\u001b[1;32m   2831\u001b[0m     verbose\u001b[39m=\u001b[39mverbose,\n\u001b[1;32m   2832\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs,\n\u001b[1;32m   2833\u001b[0m )\n",
      "File \u001b[0;32m/opt/anaconda3/envs/modelenv/lib/python3.11/site-packages/transformers/tokenization_utils_fast.py:428\u001b[0m, in \u001b[0;36mPreTrainedTokenizerFast._batch_encode_plus\u001b[0;34m(self, batch_text_or_text_pairs, add_special_tokens, padding_strategy, truncation_strategy, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose)\u001b[0m\n\u001b[1;32m    419\u001b[0m \u001b[39m# Set the truncation and padding strategy and restore the initial configuration\u001b[39;00m\n\u001b[1;32m    420\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mset_truncation_and_padding(\n\u001b[1;32m    421\u001b[0m     padding_strategy\u001b[39m=\u001b[39mpadding_strategy,\n\u001b[1;32m    422\u001b[0m     truncation_strategy\u001b[39m=\u001b[39mtruncation_strategy,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    425\u001b[0m     pad_to_multiple_of\u001b[39m=\u001b[39mpad_to_multiple_of,\n\u001b[1;32m    426\u001b[0m )\n\u001b[0;32m--> 428\u001b[0m encodings \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_tokenizer\u001b[39m.\u001b[39mencode_batch(\n\u001b[1;32m    429\u001b[0m     batch_text_or_text_pairs,\n\u001b[1;32m    430\u001b[0m     add_special_tokens\u001b[39m=\u001b[39madd_special_tokens,\n\u001b[1;32m    431\u001b[0m     is_pretokenized\u001b[39m=\u001b[39mis_split_into_words,\n\u001b[1;32m    432\u001b[0m )\n\u001b[1;32m    434\u001b[0m \u001b[39m# Convert encoding to dict\u001b[39;00m\n\u001b[1;32m    435\u001b[0m \u001b[39m# `Tokens` has type: Tuple[\u001b[39;00m\n\u001b[1;32m    436\u001b[0m \u001b[39m#                       List[Dict[str, List[List[int]]]] or List[Dict[str, 2D-Tensor]],\u001b[39;00m\n\u001b[1;32m    437\u001b[0m \u001b[39m#                       List[EncodingFast]\u001b[39;00m\n\u001b[1;32m    438\u001b[0m \u001b[39m#                    ]\u001b[39;00m\n\u001b[1;32m    439\u001b[0m \u001b[39m# with nested dimensions corresponding to batch, overflows, sequence length\u001b[39;00m\n\u001b[1;32m    440\u001b[0m tokens_and_encodings \u001b[39m=\u001b[39m [\n\u001b[1;32m    441\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_convert_encoding(\n\u001b[1;32m    442\u001b[0m         encoding\u001b[39m=\u001b[39mencoding,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    451\u001b[0m     \u001b[39mfor\u001b[39;00m encoding \u001b[39min\u001b[39;00m encodings\n\u001b[1;32m    452\u001b[0m ]\n",
      "\u001b[0;31mTypeError\u001b[0m: TextEncodeInput must be Union[TextInputSequence, Tuple[InputSequence, InputSequence]]"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from transformers import BertTokenizer, TFBertModel\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Load the data\n",
    "df = pd.read_csv('transformers_dataNew.csv')\n",
    "#df = df[df['reactions_count'] > 30]\n",
    "\n",
    "# Split the data into train, validation, and test sets\n",
    "train_data, temp_data = train_test_split(df, test_size=0.2, random_state=42)\n",
    "valid_data, test_data = train_test_split(temp_data, test_size=0.5, random_state=42)\n",
    "\n",
    "# Load the BERT tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"GroNLP/bert-base-dutch-cased\")\n",
    "\n",
    "# Tokenize and encode the text data\n",
    "train_tokens = tokenizer.batch_encode_plus(\n",
    "    train_data['content'].tolist(),\n",
    "    truncation=True,\n",
    "    padding='max_length',\n",
    "    max_length=128,\n",
    "    return_tensors='tf'\n",
    ")\n",
    "\n",
    "valid_tokens = tokenizer.batch_encode_plus(\n",
    "    valid_data['content'].tolist(),\n",
    "    truncation=True,\n",
    "    padding='max_length',\n",
    "    max_length=128,\n",
    "    return_tensors='tf'\n",
    ")\n",
    "\n",
    "test_tokens = tokenizer.batch_encode_plus(\n",
    "    test_data['content'].tolist(),\n",
    "    truncation=True,\n",
    "    padding='max_length',\n",
    "    max_length=128,\n",
    "    return_tensors='tf'\n",
    ")\n",
    "\n",
    "# Convert the tokenized input to TensorFlow Dataset\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((\n",
    "    dict(train_tokens),\n",
    "    train_data['entropy'].values\n",
    "))\n",
    "\n",
    "valid_dataset = tf.data.Dataset.from_tensor_slices((\n",
    "    dict(valid_tokens),\n",
    "    valid_data['entropy'].values\n",
    "))\n",
    "\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices((\n",
    "    dict(test_tokens),\n",
    "    test_data['entropy'].values\n",
    "))\n",
    "\n",
    "# Prepare the model input\n",
    "input_ids = tf.keras.layers.Input(shape=(128,), dtype=tf.int32, name='input_ids')\n",
    "attention_mask = tf.keras.layers.Input(shape=(128,), dtype=tf.int32, name='attention_mask')\n",
    "\n",
    "# Load the BERT model\n",
    "model = TFAutoModel.from_pretrained(\"GroNLP/bert-base-dutch-cased\")  # Tensorflow\n",
    "\n",
    "# Freeze the BERT layers\n",
    "model.trainable = False\n",
    "\n",
    "# Retrieve the BERT embeddings\n",
    "embeddings = model(input_ids, attention_mask=attention_mask)[0]\n",
    "\n",
    "# Perform pooling (average pooling in this case)\n",
    "pooled_output = tf.keras.layers.GlobalAveragePooling1D()(embeddings)\n",
    "\n",
    "# Add a dense layer for regression\n",
    "output = tf.keras.layers.Dense(1, activation='linear')(pooled_output)\n",
    "\n",
    "# Build the model\n",
    "model = tf.keras.Model(inputs=[input_ids, attention_mask], outputs=output)\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=1e-4), loss='mean_squared_error')\n",
    "\n",
    "# Train the model\n",
    "model.fit(\n",
    "    train_dataset.batch(32),\n",
    "    epochs=5,\n",
    "    validation_data=valid_dataset.batch(32)\n",
    ")\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "predictions = model.predict(test_dataset.batch(32))\n",
    "mse = mean_squared_error(test_data['entropy'].values, predictions)\n",
    "print('Mean Squared Error:', mse)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## From https://towardsdatascience.com/text-classification-with-nlp-tf-idf-vs-word2vec-vs-bert-41ff868d1794, Language model approach\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "modelenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
