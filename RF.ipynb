{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import timeit\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import os\n",
    "import scipy.stats as stats\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import string\n",
    "import nltk\n",
    "import sklearn\n",
    "\n",
    "from scipy.stats import entropy\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.optimizers import Adam\n",
    "from gensim.models import KeyedVectors\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.pipeline import Pipeline\n",
    "from transformers import AutoTokenizer, AutoModel, TFAutoModel\n",
    "from sklearn.model_selection import cross_val_score, KFold\n",
    "\n",
    "nltk.download('stopwords') # download stopwords corpus\n",
    "nltk.download('punkt') # download punkt tokenizer\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformer for reading CSV file into DataFrame\n",
    "class CSVReader(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, file_path):\n",
    "        self.file_path = file_path\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        # Read the CSV file into a DataFrame\n",
    "        df = pd.read_csv(self.file_path)\n",
    "        \n",
    "        # Return the DataFrame\n",
    "        return df\n",
    "    \n",
    "# Transformer for Gensim Dutch Word2Vec embeddings\n",
    "class GensimEmbeddings(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, model_path, embedding_size):\n",
    "        self.model_path = model_path\n",
    "        self.embedding_size = embedding_size\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        # Load the Gensim Word2Vec model\n",
    "        model = KeyedVectors.load_word2vec_format(self.model_path)\n",
    "        \n",
    "        # Initialize an empty numpy array to store the embeddings\n",
    "        embeddings = np.zeros((len(X), self.embedding_size))\n",
    "        \n",
    "        # Iterate over the words in the text and calculate the embeddings\n",
    "        for i, word in enumerate(X['prep_content']):\n",
    "            if word in model:\n",
    "                embeddings[i, :] = model[word]\n",
    "            else:\n",
    "                embeddings[i, :] = np.random.randn(self.embedding_size)\n",
    "        \n",
    "        # Add the embeddings to the DataFrame\n",
    "        for j in range(embeddings.shape[1]):\n",
    "            X[f'embedding_{j}'] = embeddings[:, j]\n",
    "        \n",
    "        return X\n",
    "\n",
    "# Transformer for TFIDF embeddings\n",
    "class TFIDFEmbeddings(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self):\n",
    "        self.tfidf_vectorizer = TfidfVectorizer()\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        self.tfidf_vectorizer.fit(X['prep_content'])\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        # Fit the TF-IDF vectorizer if it's not fitted\n",
    "        if not hasattr(self.tfidf_vectorizer, 'idf_'):\n",
    "            self.fit(X)\n",
    "        \n",
    "        # Calculate the TF-IDF embeddings\n",
    "        tfidf_embeddings = self.tfidf_vectorizer.transform(X['prep_content'])\n",
    "\n",
    "        # Add the embeddings to the DataFrame\n",
    "        for i in range(tfidf_embeddings.shape[1]):\n",
    "            X[f'tfidf_embedding_{i}'] = tfidf_embeddings[:, i].toarray().flatten()\n",
    "        \n",
    "        n_features = len(self.tfidf_vectorizer.vocabulary_)\n",
    "\n",
    "        return X, n_features\n",
    "    \n",
    "class BERTEmbeddings(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self):\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(\"GroNLP/bert-base-dutch-cased\")\n",
    "        self.model = TFAutoModel.from_pretrained(\"GroNLP/bert-base-dutch-cased\")  # Tensorflow\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        embeddings = np.zeros((len(X), 768))\n",
    "        for i, text in enumerate(X['content']):\n",
    "            input_ids = self.tokenizer.encode(str(text), add_special_tokens=True, return_tensors=\"tf\")\n",
    "            output = self.model(input_ids)\n",
    "            embeddings[i] = np.max(output.last_hidden_state.numpy(), axis=1)\n",
    "        \n",
    "        for j in range(embeddings.shape[1]):\n",
    "            X[f'embedding_{j}'] = embeddings[:, j]\n",
    "        \n",
    "        return X  \n",
    "       \n",
    "# Transformer for baseline model   \n",
    "class BaselineAnalysis(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        avg_entropy_per_source = X.groupby('name')['entropy'].mean()\n",
    "        X['avg_entropy'] = X['name'].map(avg_entropy_per_source)\n",
    "\n",
    "        mse = mean_squared_error(X['entropy'], X['avg_entropy'])\n",
    "        print(\"Baseline: Mean Squared Error:\", mse)\n",
    "\n",
    "        errors = X['entropy'] - X['avg_entropy']\n",
    "        squared_errors = errors ** 2\n",
    "        variance = np.var(squared_errors)\n",
    "        print(\"Baseline: Variance of Squared Errors:\", variance)\n",
    "\n",
    "        std = np.std(squared_errors)\n",
    "        print(\"Baseline: STD of Squared Errors:\", std)\n",
    "        \n",
    "        # Bootstrapping to calculate confidence intervals around MSE\n",
    "        n_iterations = 1000  # Number of bootstrap iterations\n",
    "        mse_bootstrapped = []\n",
    "        for _ in range(n_iterations):\n",
    "            # Resample the squared errors\n",
    "            resampled_errors = np.random.choice(squared_errors, size=len(squared_errors), replace=True)\n",
    "            # Calculate MSE from resampled errors\n",
    "            mse_bootstrapped.append(np.mean(resampled_errors))\n",
    "\n",
    "        # Calculate the lower and upper percentiles for confidence interval\n",
    "        lower_percentile = 10  # 2.5th percentile\n",
    "        upper_percentile = 90  # 97.5th percentile\n",
    "        ci_lower = np.percentile(mse_bootstrapped, lower_percentile)\n",
    "        ci_upper = np.percentile(mse_bootstrapped, upper_percentile)\n",
    "        print(f\"Baseline: MSE Confidence Interval ({lower_percentile}%, {upper_percentile}%): ({ci_lower}, {ci_upper})\")\n",
    "\n",
    "        plt.figure(figsize=(8, 6))\n",
    "        plt.hist(errors, bins=20, edgecolor='black', alpha=0.75)\n",
    "        plt.xlabel('Error')\n",
    "        plt.ylabel('Frequency')\n",
    "        plt.title('Baseline: Distribution of Errors in Test Set')\n",
    "        plt.show()\n",
    "\n",
    "        return X \n",
    "    \n",
    "# Transformer for Random Forest\n",
    "class RFTransformer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, embedding_option):\n",
    "        self.embedding_option = embedding_option\n",
    "        self.embedding_step = None\n",
    "        self.xgboost_model = None\n",
    "        self.y = None\n",
    "    \n",
    "    def fit(self, X, y= None):\n",
    "        if self.embedding_option == 'gensim':\n",
    "            self.embedding_step = GensimEmbeddings(model_path='INPUT EMBEDDING PATH', embedding_size=160)\n",
    "            X = self.embedding_step.transform(X)\n",
    "            # Prepare the input features\n",
    "            X_features = X.iloc[:, -160:]\n",
    "        elif self.embedding_option == 'tfidf':\n",
    "            self.embedding_step = TFIDFEmbeddings()\n",
    "            X, n_features = self.embedding_step.transform(X)\n",
    "            X_features = X.iloc[:, -n_features:]\n",
    "        elif self.embedding_option == 'bert':\n",
    "            self.embedding_step = BERTEmbeddings()\n",
    "            X = self.embedding_step.transform(X)\n",
    "            # Prepare the input features\n",
    "            X_features = X.iloc[:, -768:]\n",
    "        \n",
    "        # In the first step we will split the data in training and remaining dataset\n",
    "        y_target = X['entropy']\n",
    "        self.y = y_target\n",
    "        self.X_train, self.X_rem, self.y_train, self.y_rem = train_test_split(X_features , y_target, train_size=0.8)\n",
    "\n",
    "        # Now since we want the valid and test size to be equal (10% each of overall data). \n",
    "        # we have to define valid_size=0.5 (that is 50% of remaining data)\n",
    "\n",
    "        self.X_valid, self.X_test, self.y_valid, self.y_test = train_test_split(self.X_rem, self.y_rem, test_size=0.5)\n",
    "        \n",
    "        self.rf_model = RandomForestRegressor(random_state=42)\n",
    "        \n",
    "        # define the parameter grid to search\n",
    "        param_grid = {\n",
    "            'n_estimators': [50, 100, 150],\n",
    "            'max_depth': [5, 10, 15],\n",
    "            'min_samples_split': [2, 5],\n",
    "            'min_samples_leaf': [1, 3, 4, 5]\n",
    "        }\n",
    "        \n",
    "        print(\"Performing grid search...\")\n",
    "        \n",
    "        # create a GridSearchCV object and fit it to the data\n",
    "        grid_search = GridSearchCV(RandomForestRegressor(random_state=42), param_grid=param_grid, cv=10, n_jobs=-1, scoring='neg_mean_squared_error')\n",
    "        grid_search = grid_search.fit(self.X_train, self.y_train)\n",
    "        pd.DataFrame(grid_search.cv_results_)\n",
    "\n",
    "        # get the best model and print its hyperparameters\n",
    "        self.rf_model = grid_search.best_estimator_\n",
    "\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        if self.embedding_step is not None:\n",
    "            X = self.embedding_step.transform(X)\n",
    "        \n",
    "        # evaluate the model on the test set\n",
    "        y_pred = self.rf_model.predict(self.X_test)\n",
    "        test_mse = mean_squared_error(self.y_test, y_pred)\n",
    "        print(\"Random Forest: Test MSE (tuned):\", test_mse)\n",
    "        \n",
    "        # Calculate the errors\n",
    "        errors = self.y_test - y_pred\n",
    "        \n",
    "        # Calculate the variance and standard deviation of errors\n",
    "        variance = np.var(errors)\n",
    "        std = np.std(errors)\n",
    "        print('Random Forest: Variance of Errors:', variance)\n",
    "        print('Random Forest: Standard Deviation of Errors:', std)\n",
    "        \n",
    "        # Calculate the squared errors\n",
    "        squared_errors = errors ** 2\n",
    "        # Bootstrapping to calculate confidence intervals around MSE\n",
    "        n_iterations = 1000  # Number of bootstrap iterations\n",
    "        mse_bootstrapped = []\n",
    "        for _ in range(n_iterations):\n",
    "            # Resample the squared errors\n",
    "            resampled_errors = np.random.choice(squared_errors, size=len(squared_errors), replace=True)\n",
    "            # Calculate MSE from resampled errors\n",
    "            mse_bootstrapped.append(np.mean(resampled_errors))\n",
    "\n",
    "        # Calculate the lower and upper percentiles for confidence interval\n",
    "        lower_percentile = 10  # 2.5th percentile\n",
    "        upper_percentile = 90  # 97.5th percentile\n",
    "        ci_lower = np.percentile(mse_bootstrapped, lower_percentile)\n",
    "        ci_upper = np.percentile(mse_bootstrapped, upper_percentile)\n",
    "        print(f\"Random Forest: MSE Confidence Interval ({lower_percentile}%, {upper_percentile}%): ({ci_lower}, {ci_upper})\")\n",
    "\n",
    "        \n",
    "        # Plot the distribution of errors\n",
    "        plt.figure(figsize=(8, 6))\n",
    "        plt.hist(errors, bins=20, edgecolor='black', color='purple', alpha=0.75)\n",
    "        plt.xlabel('Error')\n",
    "        plt.ylabel('Frequency')\n",
    "        plt.title('Random Forest: Distribution of Errors in Test Set')\n",
    "        plt.show()\n",
    "        \n",
    "        # Plot the distribution of entropy scores in the Train set\n",
    "        plt.figure(figsize=(8, 6))\n",
    "        plt.hist(self.y_train, bins=20, edgecolor='black', color='purple', alpha=0.75)\n",
    "        plt.xlabel('Entropy')\n",
    "        plt.ylabel('Frequency')\n",
    "        plt.title('Random Forest: Distribution of Entropy Scores in Train Set')\n",
    "        plt.show()\n",
    "        \n",
    "        # Plot the distribution of entropy scores in the Test set\n",
    "        plt.figure(figsize=(8, 6))\n",
    "        plt.hist(self.y_test, bins=20, edgecolor='black', color='purple', alpha=0.75)\n",
    "        plt.xlabel('Entropy')\n",
    "        plt.ylabel('Frequency')\n",
    "        plt.title('Random Forest: Distribution of Entropy Scores in Test Set')\n",
    "        plt.show()\n",
    "    \n",
    "        return X\n",
    "\n",
    "chosen_embedding = 'bert'  \n",
    "\n",
    "pipeline = Pipeline([\n",
    "    ('csv_reader', CSVReader('INPUT FILE PATH')),\n",
    "    ('baseline_analysis', BaselineAnalysis()),\n",
    "    ('RF', RFTransformer(embedding_option=chosen_embedding)),\n",
    "    \n",
    "])\n",
    "\n",
    "embedding_options = {\n",
    "    'gensim': GensimEmbeddings(model_path='INPUT EMBEDDING PATH', embedding_size=160),\n",
    "    'tfidf': TFIDFEmbeddings(),\n",
    "    'bert': BERTEmbeddings(),\n",
    "}\n",
    "\n",
    "pipeline.set_params(RF__embedding_option=chosen_embedding)\n",
    "df_transformed = pipeline.fit_transform(None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "modelenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
