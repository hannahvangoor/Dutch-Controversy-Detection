{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "b3e45988",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/Hannah/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /Users/Hannah/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import timeit\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.stats import entropy\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import os\n",
    "\n",
    "# Clean and tokenize text\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import string\n",
    "\n",
    "nltk.download('stopwords') # download stopwords corpus\n",
    "nltk.download('punkt') # download punkt tokenizer\n",
    "\n",
    "# For linear regression\n",
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ac09fc65",
   "metadata": {},
   "source": [
    "## TRANSFORMERS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d15e89a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd /Users/Hannah/Documents/VU/Msc/Thesis/Coding/Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "635f04ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('Indicator-Desc_DataNew.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbd00db7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModel, TFAutoModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6833545",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"GroNLP/bert-base-dutch-cased\")\n",
    "model = TFAutoModel.from_pretrained(\"GroNLP/bert-base-dutch-cased\")  # Tensorflow\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8e77f0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize an empty array to store the embeddings\n",
    "embeddings = np.zeros((len(df), 768))\n",
    "\n",
    "# Loop through the dataframe column and generate embeddings\n",
    "for i, text in enumerate(df['content']):\n",
    "    # Encode the text using the tokenizer\n",
    "    input_ids = tokenizer.encode(str(text), add_special_tokens=True, return_tensors=\"tf\")\n",
    "    # Generate the embeddings using the model\n",
    "    output = model(input_ids)\n",
    "    # Extract the embeddings from the output and flatten using max-pooling\n",
    "    embeddings[i] = np.max(output.last_hidden_state.numpy(), axis=1)\n",
    "    \n",
    "# Add the embeddings to the dataframe\n",
    "for j in range(768):\n",
    "    df[f'embedding_{j}'] = embeddings[:, j]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69fa3c40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the embeddings to the dataframe\n",
    "df['embeddings'] = list(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63cb0809",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('transformers_dataNew.csv', index=False)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "05a64fd5",
   "metadata": {},
   "source": [
    "## TF IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "010dbbb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd /Users/Hannah/Documents/VU/Msc/Thesis/Coding/Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a87dc6e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('Indicator-Desc_Data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ba7d672",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_and_tokenize(text):\n",
    "    # Remove punctuation\n",
    "    text = text.translate(str.maketrans(\"\", \"\", string.punctuation))\n",
    "    \n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Tokenize the text\n",
    "    tokens = word_tokenize(text)\n",
    "    \n",
    "    # Remove stopwords\n",
    "    stop_words = set(stopwords.words('dutch'))\n",
    "    tokens = [token for token in tokens if not token in stop_words]\n",
    "    \n",
    "    # Join the tokens back into a single string\n",
    "    text = \" \".join(tokens)\n",
    "    \n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "961ecaea",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['content'] = df['content'].astype(str) # convert column to string data type\n",
    "df['prep_content'] = df['content'].apply(clean_and_tokenize)\n",
    "df['prep_content'] = df['prep_content'].astype(str) # convert column to string data type\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c62e6798",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Tf-Idf (advanced variant of BoW)\n",
    "vectorizer = TfidfVectorizer(max_features=10000, ngram_range=(1,2))\n",
    "\n",
    "# Fit and transform the vectorizer on the text column\n",
    "X = vectorizer.fit_transform(df['prep_content'])\n",
    "\n",
    "# Convert the sparse matrix to a dense numpy array\n",
    "X_array = X.toarray()\n",
    "\n",
    "# Create a new dataframe with the tf-idf features and the original column names\n",
    "tfidf_df = pd.DataFrame(X_array)\n",
    "\n",
    "# Add the new dataframe as columns to the original dataframe\n",
    "df = pd.concat([df, tfidf_df], axis=1)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "76bd3e6d",
   "metadata": {},
   "source": [
    "## GLOVE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78966e9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import fasttext as ft\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import gensim.downloader as api\n",
    "from gensim import corpora\n",
    "from gensim import models\n",
    "from gensim.models import KeyedVectors\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cb89567",
   "metadata": {},
   "outputs": [],
   "source": [
    "glove_model = gensim.models.KeyedVectors.load_word2vec_format('glove.6B.50d.txt', binary=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acb7a0d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_text_embeddings(texts, glove_model):\n",
    "    \"\"\"\n",
    "    Convert input texts into a matrix of GloVe embeddings.\n",
    "    \"\"\"\n",
    "    embeddings = []\n",
    "    for text in texts:\n",
    "        # get GloVe embedding for each word\n",
    "        word_embeddings = []\n",
    "        for word in text:\n",
    "            # get GloVe embedding for the word\n",
    "            if word in glove_model:\n",
    "                word_embedding = glove_model[word]\n",
    "                word_embeddings.append(word_embedding)\n",
    "        # combine word embeddings into a single sentence embedding\n",
    "        if word_embeddings:\n",
    "            sentence_embedding = np.mean(word_embeddings, axis=0)\n",
    "        else:\n",
    "            sentence_embedding = np.zeros(glove_model.vector_size)\n",
    "        embeddings.append(sentence_embedding)\n",
    "    return np.array(embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01ec243a",
   "metadata": {},
   "source": [
    "## Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "6a9ffc2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df = pd.read_csv('transformers_data-High_RC.csv') # To read in the csv file that only has posts > 40 reactions\n",
    "df = pd.read_csv('transformers_dataNew.csv') # To read in the csv file with all posts\n",
    "df = df[df['reactions_count'] > 30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "096f5554",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split into train and test sets\n",
    "# Split the data into X and y\n",
    "X = df.iloc[:, -769:-1]\n",
    "#X = df['prep_content']\n",
    "df.dropna(subset=['entropy'], inplace=True)\n",
    "y = df['entropy']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "9b47e099",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error: 0.1211\n",
      "Mean Absolute Error: 0.28\n",
      "R-squared: -0.03\n"
     ]
    }
   ],
   "source": [
    "# get text embeddings for train and test sets\n",
    "#X_train_embeddings = get_text_embeddings(X_train, glove_model)\n",
    "#X_test_embeddings = get_text_embeddings(X_test, glove_model)\n",
    "# define and train a linear regression model\n",
    "model = LinearRegression()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# make predictions on the test set\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# evaluate model performance\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "# ADD LOSS FUNCTION / INSIGHTS? maybe check overfitting with validation set. \n",
    "print(f'Mean Squared Error: {mse:.4f}')\n",
    "print(f'Mean Absolute Error: {mae:.2f}')\n",
    "print(f'R-squared: {r2:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "21a978b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.5220547 , 0.41011461, 0.50753803, ..., 0.46926745, 0.2164918 ,\n",
       "       0.60698682])"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7b9fa2e",
   "metadata": {},
   "source": [
    "## Explainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10ac1da5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from lime.lime_text import LimeTextExplainer\n",
    "\n",
    "# Convert word embeddings to text representation\n",
    "def embeddings_to_text(embeddings):\n",
    "    # Convert embeddings to a string representation\n",
    "    text = ' '.join([str(val) for val in embeddings])\n",
    "    return text\n",
    "\n",
    "# Define LimeTextExplainer\n",
    "explainer = LimeTextExplainer()\n",
    "\n",
    "# Convert the sample's word embeddings to text representation\n",
    "sample_text = X.apply(embeddings_to_text, axis=1)\n",
    "#sample_text = embeddings_to_text(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d4d5389",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select a sample from your data\n",
    "sample = sample_text.values[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4501d47",
   "metadata": {},
   "outputs": [],
   "source": [
    "from lime.lime_text import LimeTextExplainer\n",
    "\n",
    "# Convert word embeddings to text representation\n",
    "def embeddings_to_text(embeddings):\n",
    "    # Convert embeddings to a string representation\n",
    "    text = ' '.join([str(val) for val in embeddings])\n",
    "    return text\n",
    "\n",
    "# Define class names for regression\n",
    "class_names = ['entropy']\n",
    "\n",
    "# Define LimeTextExplainer\n",
    "explainer = LimeTextExplainer()\n",
    "\n",
    "\n",
    "# Convert the sample's word embeddings to text representation\n",
    "sample_text = X.apply(embeddings_to_text, axis=1)\n",
    "#sample_text = embeddings_to_text(sample)\n",
    "\n",
    "sample_str = sample_text.values[0]  # Assuming 'sample_str' is a string\n",
    "sample_array = np.array(sample_str.split(','), dtype=float)  # Split the string and convert substrings to floats\n",
    "sample_reshaped = sample_array.reshape(1, -1)  # Reshape the array as needed\n",
    "\n",
    "\n",
    "# Explain the instance using LimeTextExplainer\n",
    "exp = explainer.explain_instance(sample_reshaped, model.predict, num_features=6)\n",
    "\n",
    "# Generate the explanation in HTML format\n",
    "html = exp.as_html()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb224266",
   "metadata": {},
   "outputs": [],
   "source": [
    "from lime.lime_text import LimeTextExplainer\n",
    "\n",
    "# Define LimeTextExplainer\n",
    "explainer = LimeTextExplainer()\n",
    "\n",
    "# Select a random instance to explain\n",
    "#instance_idx = np.random.randint(len(X))\n",
    "instance = X[5]\n",
    "exp = explainer.explain_instance(instance, model.predict, num_features=10)\n",
    "\n",
    "# Print the explanation\n",
    "print(exp.as_list())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b495b7a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
