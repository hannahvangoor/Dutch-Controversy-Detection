{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3e45988",
   "metadata": {},
   "outputs": [],
   "source": [
    "import timeit\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.stats import entropy\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "import os\n",
    "from sklearn.pipeline import Pipeline\n",
    "from transformers import AutoTokenizer, AutoModel, TFAutoModel\n",
    "from sklearn.linear_model import Ridge, Lasso\n",
    "from gensim.models import KeyedVectors\n",
    "import scipy.stats as stats\n",
    "\n",
    "\n",
    "\n",
    "# Clean and tokenize text\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import string\n",
    "from sklearn.model_selection import cross_val_score, KFold\n",
    "\n",
    "nltk.download('stopwords') # download stopwords corpus\n",
    "nltk.download('punkt') # download punkt tokenizer\n",
    "\n",
    "# For linear regression\n",
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01ec243a",
   "metadata": {},
   "source": [
    "## Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "096f5554",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformer for reading CSV file into DataFrame\n",
    "class CSVReader(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, file_path):\n",
    "        self.file_path = file_path\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        # Read the CSV file into a DataFrame\n",
    "        df = pd.read_csv(self.file_path)\n",
    "        \n",
    "        # Return the DataFrame\n",
    "        return df\n",
    "\n",
    "# Transformer for Gensim Dutch Word2Vec embeddings\n",
    "class GensimEmbeddings(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, model_path, embedding_size):\n",
    "        self.model_path = model_path\n",
    "        self.embedding_size = embedding_size\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        # Load the Gensim Word2Vec model\n",
    "        model = KeyedVectors.load_word2vec_format(self.model_path)\n",
    "        \n",
    "        # Initialize an empty numpy array to store the embeddings\n",
    "        embeddings = np.zeros((len(X), self.embedding_size))\n",
    "        \n",
    "        # Iterate over the words in the text and calculate the embeddings\n",
    "        for i, word in enumerate(X['prep_content']):\n",
    "            if word in model:\n",
    "                embeddings[i, :] = model[word]\n",
    "            else:\n",
    "                embeddings[i, :] = np.random.randn(self.embedding_size)\n",
    "        \n",
    "        # Add the embeddings to the DataFrame\n",
    "        for j in range(embeddings.shape[1]):\n",
    "            X[f'embedding_{j}'] = embeddings[:, j]\n",
    "        \n",
    "        return X\n",
    "\n",
    "# Transformer for TFIDF embeddings\n",
    "class TFIDFEmbeddings(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self):\n",
    "        self.tfidf_vectorizer = TfidfVectorizer()\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        self.tfidf_vectorizer.fit(X['prep_content'])\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        # Fit the TF-IDF vectorizer if it's not fitted\n",
    "        if not hasattr(self.tfidf_vectorizer, 'idf_'):\n",
    "            self.fit(X)\n",
    "        \n",
    "        # Calculate the TF-IDF embeddings\n",
    "        tfidf_embeddings = self.tfidf_vectorizer.transform(X['prep_content'])\n",
    "\n",
    "        # Add the embeddings to the DataFrame\n",
    "        for i in range(tfidf_embeddings.shape[1]):\n",
    "            X[f'tfidf_embedding_{i}'] = tfidf_embeddings[:, i].toarray().flatten()\n",
    "        \n",
    "        n_features = len(self.tfidf_vectorizer.vocabulary_)\n",
    "\n",
    "        return X, n_features\n",
    "    \n",
    "class BERTEmbeddings(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self):\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(\"GroNLP/bert-base-dutch-cased\")\n",
    "        self.model = TFAutoModel.from_pretrained(\"GroNLP/bert-base-dutch-cased\")  # Tensorflow\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        embeddings = np.zeros((len(X), 768))\n",
    "        for i, text in enumerate(X['content']):\n",
    "            input_ids = self.tokenizer.encode(str(text), add_special_tokens=True, return_tensors=\"tf\")\n",
    "            output = self.model(input_ids)\n",
    "            embeddings[i] = np.max(output.last_hidden_state.numpy(), axis=1)\n",
    "        \n",
    "        for j in range(embeddings.shape[1]):\n",
    "            X[f'embedding_{j}'] = embeddings[:, j]\n",
    "        \n",
    "        return X    \n",
    " # Transformer for baseline model   \n",
    "class BaselineAnalysis(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        avg_entropy_per_source = X.groupby('name')['entropy'].mean()\n",
    "        X['avg_entropy'] = X['name'].map(avg_entropy_per_source)\n",
    "\n",
    "        mse = mean_squared_error(X['entropy'], X['avg_entropy'])\n",
    "        print(\"Baseline: Mean Squared Error:\", mse)\n",
    "\n",
    "        errors = X['entropy'] - X['avg_entropy']\n",
    "        squared_errors = errors ** 2        \n",
    "        variance = np.var(squared_errors)\n",
    "        print(\"Baseline: Variance of Squared Errors:\", variance)\n",
    "\n",
    "        std = np.std(squared_errors)\n",
    "        print(\"Baseline: STD of Squared Errors:\", std)\n",
    "        \n",
    "        # Bootstrapping to calculate confidence intervals around MSE\n",
    "        n_iterations = 1000  # Number of bootstrap iterations\n",
    "        mse_bootstrapped = []\n",
    "        for _ in range(n_iterations):\n",
    "            # Resample the squared errors\n",
    "            resampled_errors = np.random.choice(squared_errors, size=len(squared_errors), replace=True)\n",
    "            # Calculate MSE from resampled errors\n",
    "            mse_bootstrapped.append(np.mean(resampled_errors))\n",
    "\n",
    "        # Calculate the lower and upper percentiles for confidence interval\n",
    "        lower_percentile = 10  # 2.5th percentile\n",
    "        upper_percentile = 90  # 97.5th percentile\n",
    "        ci_lower = np.percentile(mse_bootstrapped, lower_percentile)\n",
    "        ci_upper = np.percentile(mse_bootstrapped, upper_percentile)\n",
    "        print(f\"Baseline: MSE Confidence Interval ({lower_percentile}%, {upper_percentile}%): ({ci_lower}, {ci_upper})\")\n",
    "\n",
    "        plt.figure(figsize=(8, 6))\n",
    "        plt.hist(errors, bins=20, edgecolor='black', alpha=0.75)\n",
    "        plt.xlabel('Error')\n",
    "        plt.ylabel('Frequency')\n",
    "        plt.title('Baseline: Distribution of Errors in Test Set')\n",
    "        plt.show()\n",
    "\n",
    "        return X \n",
    "      \n",
    " # Transformer for Linear Regression    \n",
    "class LinearRegressionAnalysis(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, embedding_option, regularization=None, alpha=1.0):\n",
    "        self.embedding_option = embedding_option\n",
    "        self.embedding_step = None\n",
    "        self.regularization = regularization\n",
    "        self.alpha = alpha\n",
    "        self.model = None\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        \n",
    "\n",
    "        # Prepare the input features based on the chosen embedding option\n",
    "        if self.embedding_option == 'gensim':\n",
    "            self.embedding_step = GensimEmbeddings(model_path='INPUT EMBEDDING PATH', embedding_size=160)\n",
    "            X = self.embedding_step.transform(X)\n",
    "            # Prepare the input features\n",
    "            X_features = X.iloc[:, -160:]\n",
    "        elif self.embedding_option == 'tfidf':\n",
    "            self.embedding_step = TFIDFEmbeddings()\n",
    "            X, n_features = self.embedding_step.transform(X)\n",
    "            X_features = X.iloc[:, -n_features:]\n",
    "        elif self.embedding_option == 'bert':\n",
    "            self.embedding_step = BERTEmbeddings()\n",
    "            X = self.embedding_step.transform(X)\n",
    "            # Prepare the input features\n",
    "            X_features = X.iloc[:, -768:]\n",
    "        \n",
    "        # Define and train the linear regression model\n",
    "        if self.regularization == 'ridge':\n",
    "            self.model = Ridge(alpha=self.alpha)\n",
    "        elif self.regularization == 'lasso':\n",
    "            self.model = Lasso(alpha=self.alpha)\n",
    "        else:\n",
    "            self.model = LinearRegression()\n",
    "            \n",
    "        # Prepare the target variable\n",
    "        y_target = X['entropy']\n",
    "        \n",
    "        # Split the data into training and test sets\n",
    "        self.X_train, self.X_test, self.y_train, self.y_test = train_test_split(X_features, y_target, test_size=0.2, random_state=42)\n",
    "        \n",
    "        # Perform cross-validation\n",
    "        cv = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "        scores = cross_val_score(self.model, X_features, y_target, scoring='neg_mean_squared_error', cv=cv)\n",
    "\n",
    "        # Print the cross-validation scores\n",
    "        print('Cross-Validation Scores:', scores)\n",
    "        print('Average Cross-Validation MSE:', -np.mean(scores))\n",
    "        print('Variance Cross-Validation MSE:', np.var(scores))\n",
    "        \n",
    "        self.model.fit(self.X_train, self.y_train)\n",
    "        \n",
    "        return self\n",
    "        \n",
    "    def transform(self, X):\n",
    "        if self.embedding_step is not None:\n",
    "            X = self.embedding_step.transform(X)\n",
    "    \n",
    "        # Make predictions on the test set\n",
    "        y_pred = self.model.predict(self.X_test)\n",
    "\n",
    "        # Calculate the errors\n",
    "        errors = self.y_test - y_pred\n",
    "        \n",
    "        # Plot the distribution of errors\n",
    "        plt.figure(figsize=(8, 6))\n",
    "        plt.hist(errors, bins=20, edgecolor='black', color='red', alpha=0.75)\n",
    "        plt.xlabel('Error')\n",
    "        plt.ylabel('Frequency')\n",
    "        plt.title('Linear Regression: Distribution of Errors in Test Set')\n",
    "        plt.show()\n",
    "        \n",
    "        # Plot the distribution of entropy scores in the Train set\n",
    "        plt.figure(figsize=(8, 6))\n",
    "        plt.hist(self.y_train, bins=20, edgecolor='black', color='red', alpha=0.75)\n",
    "        plt.xlabel('Entropy')\n",
    "        plt.ylabel('Frequency')\n",
    "        plt.title('Linear Regression: Distribution of Entropy Scores in Train Set')\n",
    "        plt.show()\n",
    "        \n",
    "        # Plot the distribution of entropy scores in the Test set\n",
    "        plt.figure(figsize=(8, 6))\n",
    "        plt.hist(self.y_test, bins=20, edgecolor='black', color='red', alpha=0.75)\n",
    "        plt.xlabel('Entropy')\n",
    "        plt.ylabel('Frequency')\n",
    "        plt.title('Linear Regression: Distribution of Entropy Scores in Test Set')\n",
    "        plt.show()\n",
    "        \n",
    "        # Evaluate model performance\n",
    "        mse = mean_squared_error(self.y_test, y_pred)\n",
    "        r2 = r2_score(self.y_test, y_pred)\n",
    "        var = np.var(errors)\n",
    "        std = np.std(errors)\n",
    "        \n",
    "        # Print the evaluation metrics\n",
    "        print(f'Linear Regression: Mean Squared Error: {mse:.4f}')\n",
    "        print(f'Linear Regression: R-squared: {r2:.2f}')\n",
    "        print(f'Linear Regression: Variance of Errors: {var:.4f}')\n",
    "        print(f'Linear Regression: Standard Deviation of Errors: {std:.4f}')\n",
    "        \n",
    "        # Bootstrapping to calculate confidence intervals around MSE\n",
    "        # Calculate the squared errors\n",
    "        squared_errors = errors**2\n",
    "        n_iterations = 1000  # Number of bootstrap iterations\n",
    "        mse_bootstrapped = []\n",
    "        for _ in range(n_iterations):\n",
    "            # Resample the squared errors\n",
    "            resampled_errors = np.random.choice(squared_errors, size=len(squared_errors), replace=True)\n",
    "            # Calculate MSE from resampled errors\n",
    "            mse_bootstrapped.append(np.mean(resampled_errors))\n",
    "\n",
    "        # Calculate the lower and upper percentiles for confidence interval\n",
    "        lower_percentile = 10  # 2.5th percentile\n",
    "        upper_percentile = 90  # 97.5th percentile\n",
    "        ci_lower = np.percentile(mse_bootstrapped, lower_percentile)\n",
    "        ci_upper = np.percentile(mse_bootstrapped, upper_percentile)\n",
    "        print(f\"Linear Regression: MSE Confidence Interval ({lower_percentile}%, {upper_percentile}%): ({ci_lower}, {ci_upper})\")\n",
    "        \n",
    "        return X\n",
    "\n",
    "chosen_embedding = 'bert'  # INPUT CHOSEN EMBEDDING HERE\n",
    "chosen_regularization = 'lasso'  # INPUT CHOSEN REGULARIZATION ('ridge' or 'lasso')\n",
    "alpha_value = 1.0  # INPUT ALPHA VALUE HERE\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    ('csv_reader', CSVReader('INPUT FILE PATH')), # INPUT FILE\n",
    "    ('baseline_analysis', BaselineAnalysis()),\n",
    "    ('linear_regression', LinearRegressionAnalysis(embedding_option=chosen_embedding, regularization=chosen_regularization, alpha=alpha_value)),\n",
    "])\n",
    "\n",
    "embedding_options = {\n",
    "    'gensim': GensimEmbeddings(model_path='INPUT EMBEDDING PATH', embedding_size=160), #INPUT WORD2VEC MODEL PATH\n",
    "    'tfidf': TFIDFEmbeddings(),\n",
    "    'bert': BERTEmbeddings(),\n",
    "}\n",
    "\n",
    "pipeline.set_params(linear_regression__embedding_option=chosen_embedding)\n",
    "df_transformed = pipeline.fit_transform(None)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
